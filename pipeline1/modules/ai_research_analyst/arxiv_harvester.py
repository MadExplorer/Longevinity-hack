"""
ArXiv Harvester - —Å–±–æ—Ä—â–∏–∫ —Å—Ç–∞—Ç–µ–π —Å arXiv API
"""

import requests
import xml.etree.ElementTree as ET
import time
import logging
from typing import List, Optional, Dict, Tuple
from urllib.parse import quote
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

from .models import Paper
from .config import (
    ARXIV_BASE_URL,
    MAX_PAPERS_PER_QUERY,
    ARXIV_REQUEST_TIMEOUT,
    ARXIV_RATE_LIMIT_DELAY
)


logger = logging.getLogger(__name__)


class ArxivHarvester:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–µ–π —Å arXiv"""
    
    def __init__(self):
        self.base_url = ARXIV_BASE_URL
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'AI-Research-Analyst/1.0 (your-email@example.com)'
        })
    
    def search_papers(self, query: str, max_results: int = MAX_PAPERS_PER_QUERY) -> List[Paper]:
        """
        –ò—â–µ—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É –≤ arXiv
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ Paper
        """
        logger.info(f"–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            encoded_query = quote(query)
            url = f"{self.base_url}?search_query=all:{encoded_query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending"
            
            # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ arXiv API
            response = self.session.get(url, timeout=ARXIV_REQUEST_TIMEOUT)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º XML –æ—Ç–≤–µ—Ç
            papers = self._parse_arxiv_response(response.text)
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(papers)} —Å—Ç–∞—Ç–µ–π")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è rate limit
            time.sleep(ARXIV_RATE_LIMIT_DELAY)
            
            return papers
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å—Ç–∞—Ç–µ–π: {e}")
            return []
    
    def search_papers_parallel(self, queries: List[str], max_results: int = MAX_PAPERS_PER_QUERY, max_workers: int = 5) -> Dict[str, List[Paper]]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å
            max_workers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∑–∞–ø—Ä–æ—Å: —Å–ø–∏—Å–æ–∫_—Å—Ç–∞—Ç–µ–π}
        """
        logger.info(f"üöÄ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–∞–º —Å {max_workers} –ø–æ—Ç–æ–∫–∞–º–∏")
        
        results = {}
        rate_limit_lock = Lock()  # –î–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è rate limit
        
        def search_single_query(query: str) -> Tuple[str, List[Paper]]:
            """–ü–æ–∏—Å–∫ –ø–æ –æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É —Å rate limiting"""
            try:
                # –°–æ–±–ª—é–¥–∞–µ–º rate limit
                with rate_limit_lock:
                    time.sleep(ARXIV_RATE_LIMIT_DELAY)
                
                logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query[:50]}...")
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
                encoded_query = quote(query)
                url = f"{self.base_url}?search_query=all:{encoded_query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending"
                
                # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ arXiv API
                response = self.session.get(url, timeout=ARXIV_REQUEST_TIMEOUT)
                response.raise_for_status()
                
                # –ü–∞—Ä—Å–∏–º XML –æ—Ç–≤–µ—Ç
                papers = self._parse_arxiv_response(response.text)
                
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(papers)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
                return query, papers
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query[:50]}...': {e}")
                return query, []
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏
            future_to_query = {
                executor.submit(search_single_query, query): query 
                for query in queries
            }
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            for future in as_completed(future_to_query):
                query, papers = future.result()
                results[query] = papers
        
        total_papers = sum(len(papers) for papers in results.values())
        logger.info(f"üéØ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω! –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {total_papers} —Å—Ç–∞—Ç–µ–π")
        
        return results
    
    def _parse_arxiv_response(self, xml_content: str) -> List[Paper]:
        """
        –ü–∞—Ä—Å–∏—Ç XML –æ—Ç–≤–µ—Ç –æ—Ç arXiv API
        
        Args:
            xml_content: XML –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç arXiv
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ Paper
        """
        papers = []
        
        try:
            root = ET.fromstring(xml_content)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º namespace
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã entry (—Å—Ç–∞—Ç—å–∏)
            entries = root.findall('atom:entry', ns)
            
            for entry in entries:
                paper = self._parse_entry(entry, ns)
                if paper:
                    papers.append(paper)
                    
        except ET.ParseError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ arXiv: {e}")
            
        return papers
    
    def _parse_entry(self, entry: ET.Element, ns: dict) -> Optional[Paper]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç—å–∏
        
        Args:
            entry: XML —ç–ª–µ–º–µ–Ω—Ç entry
            ns: Namespace –¥–ª—è XML
            
        Returns:
            –û–±—ä–µ–∫—Ç Paper –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID —Å—Ç–∞—Ç—å–∏
            id_elem = entry.find('atom:id', ns)
            paper_id = id_elem.text.split('/')[-1] if id_elem is not None else ""
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_elem = entry.find('atom:published', ns)
            published_date = published_elem.text[:10] if published_elem is not None else ""
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text.strip() if title_elem is not None else ""
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
            summary_elem = entry.find('atom:summary', ns)
            summary = summary_elem.text.strip() if summary_elem is not None else ""
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤
            authors = []
            author_elems = entry.findall('atom:author', ns)
            for author_elem in author_elems:
                name_elem = author_elem.find('atom:name', ns)
                if name_elem is not None:
                    authors.append(name_elem.text)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL
            link_elems = entry.findall('atom:link', ns)
            url = ""
            for link in link_elems:
                if link.get('title') == 'pdf':
                    url = link.get('href', '')
                    break
            
            if not url:  # –ï—Å–ª–∏ PDF —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Å—ã–ª–∫—É
                for link in link_elems:
                    if link.get('rel') == 'alternate':
                        url = link.get('href', '')
                        break
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Paper
            paper = Paper(
                id=paper_id,
                published_date=published_date,
                title=title,
                summary=summary,
                authors=authors,
                url=url
            )
            
            return paper
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏: {e}")
            return None
    
    def harvest_multiple_queries(self, queries: List[str]) -> List[Paper]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        """
        all_papers = []
        seen_ids = set()
        
        for query in queries:
            papers = self.search_papers(query)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
            for paper in papers:
                if paper.id not in seen_ids:
                    all_papers.append(paper)
                    seen_ids.add(paper.id)
        
        logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(all_papers)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return all_papers 