"""
Orchestrator - –≥–ª–∞–≤–Ω—ã–π —É–ø—Ä–∞–≤–ª—è—é—â–∏–π –º–æ–¥—É–ª—å AI Research Analyst
"""

import logging
from typing import List, Tuple
from tqdm import tqdm

from .models import Paper, RankedPaper
from .query_strategist import QueryStrategist
from .arxiv_harvester import ArxivHarvester
from .paper_evaluator import PaperEvaluator
from .final_synthesizer import FinalSynthesizer
from .config import TARGET_PAPER_COUNT, MIN_SCORE_THRESHOLD


logger = logging.getLogger(__name__)


class ResearchOrchestrator:
    """–ì–ª–∞–≤–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self):
        self.query_strategist = QueryStrategist()
        self.arxiv_harvester = ArxivHarvester()
        self.paper_evaluator = PaperEvaluator()
        self.final_synthesizer = FinalSynthesizer()
        
        self.validated_papers: List[RankedPaper] = []
        self.all_papers_analyzed: List[RankedPaper] = []
    
    def run_research_pipeline(self, research_topic: str, target_count: int = TARGET_PAPER_COUNT) -> str:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        
        Args:
            research_topic: –¢–µ–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            target_count: –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
            
        Returns:
            –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ: '{research_topic}'")
        logger.info(f"üéØ –¶–µ–ª—å: {target_count} –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π (–ø–æ—Ä–æ–≥ –æ—Ü–µ–Ω–∫–∏: {MIN_SCORE_THRESHOLD})")
        
        try:
            # –®–∞–≥ 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            logger.info("\nüìù –®–∞–≥ 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
            queries = self.query_strategist.generate_queries(research_topic)
            logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
            
            # –®–∞–≥ 2: –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Å–±–æ—Ä–∞ –∏ –æ—Ü–µ–Ω–∫–∏ —Å—Ç–∞—Ç–µ–π
            logger.info(f"\nüîÑ –®–∞–≥ 2: –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Å–±–æ—Ä–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞...")
            self._research_loop(research_topic, queries, target_count)
            
            # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
            logger.info(f"\nüìä –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
            report = self._create_final_report(research_topic)
            
            logger.info(f"‚ú® –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            self._print_summary()
            
            return report
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
            return self._create_error_report(research_topic, str(e))
    
    def _research_loop(self, research_topic: str, queries: List[str], target_count: int):
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Ü–∏–∫–ª –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–∞–º")
        
        with tqdm(desc="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –æ—Ü–µ–Ω–∫–∞", 
                 total=target_count, 
                 unit="—Å—Ç–∞—Ç–µ–π") as pbar:
            
            # –®–∞–≥ 1: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º —Å—Ä–∞–∑—É
            logger.info(f"\nüîç –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π...")
            search_results = self.arxiv_harvester.search_papers_parallel(queries, max_workers=5)
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
            all_found_papers = []
            for query, papers in search_results.items():
                if papers:
                    logger.info(f"üìä –ó–∞–ø—Ä–æ—Å '{query[:50]}...' -> {len(papers)} —Å—Ç–∞—Ç–µ–π")
                    all_found_papers.extend(papers)
                else:
                    logger.warning(f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—Å '{query[:50]}...' -> 0 —Å—Ç–∞—Ç–µ–π")
            
            if not all_found_papers:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –ø–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º")
                return
            
            logger.info(f"üìà –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_found_papers)} —Å—Ç–∞—Ç–µ–π")
            
            # –®–∞–≥ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
            logger.info(f"\nüîß –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
            unique_papers = self._remove_duplicates(all_found_papers)
            logger.info(f"üìä –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(unique_papers)} —Å—Ç–∞—Ç–µ–π")
            
            new_papers = self._filter_new_papers(unique_papers)
            if not new_papers:
                logger.info(f"‚ÑπÔ∏è –í—Å–µ —Å—Ç–∞—Ç—å–∏ —É–∂–µ –±—ã–ª–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
                return
            
            logger.info(f"üìä –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(new_papers)}")
            
            # –®–∞–≥ 3: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ç–∞—Ç–µ–π
            logger.info(f"\nüìä –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ {len(new_papers)} —Å—Ç–∞—Ç–µ–π...")
            
            # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç–µ–π
            if len(new_papers) > 30:
                # –î–ª—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É –±–∞—Ç—á–∞–º–∏
                ranked_papers = self.paper_evaluator.evaluate_papers_parallel(
                    new_papers, 
                    research_topic, 
                    batch_size=10, 
                    max_workers=3
                )
            else:
                # –î–ª—è –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –±–∞—Ç—á–µ–≤—É—é –æ—Ü–µ–Ω–∫—É
                ranked_papers = self.paper_evaluator.evaluate_papers(new_papers, research_topic)
            
            # –®–∞–≥ 4: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.all_papers_analyzed.extend(ranked_papers)
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
            newly_validated = self.paper_evaluator.filter_validated_papers(ranked_papers)
            
            if newly_validated:
                self.validated_papers.extend(newly_validated)
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏–µ
                self.validated_papers.sort(key=lambda x: x.score, reverse=True)
                self.validated_papers = self.validated_papers[:target_count * 2]  # –î–µ—Ä–∂–∏–º –∑–∞–ø–∞—Å
                
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(newly_validated)} –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
                pbar.update(min(len(newly_validated), target_count - pbar.n))
            else:
                logger.warning(f"‚ö†Ô∏è –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –ª–æ–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            current_validated = len(self.validated_papers)
            logger.info(f"üìà –ò—Ç–æ–≥–æ: {current_validated}/{target_count} –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –æ–±—Ä–µ–∑–∫–∞ –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        self.validated_papers.sort(key=lambda x: x.score, reverse=True)
        self.validated_papers = self.validated_papers[:target_count]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–Ω–≥–∏
        for rank, paper in enumerate(self.validated_papers, 1):
            paper.rank = rank
    
    def _remove_duplicates(self, papers: List[Paper]) -> List[Paper]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç–∞—Ç–µ–π –ø–æ ID"""
        seen_ids = set()
        unique_papers = []
        
        for paper in papers:
            if paper.id not in seen_ids:
                seen_ids.add(paper.id)
                unique_papers.append(paper)
        
        return unique_papers
    
    def _filter_new_papers(self, papers: List[Paper]) -> List[Paper]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏, –∏—Å–∫–ª—é—á–∞—è —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ"""
        analyzed_ids = {p.id for p in self.all_papers_analyzed}
        return [p for p in papers if p.id not in analyzed_ids]
    
    def _create_final_report(self, research_topic: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç"""
        if not self.validated_papers:
            return self._create_no_results_report(research_topic)
        
        return self.final_synthesizer.create_report(
            research_topic=research_topic,
            top_papers=self.validated_papers,
            total_analyzed=len(self.all_papers_analyzed)
        )
    
    def _create_no_results_report(self, research_topic: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –æ—Ç—á–µ—Ç –¥–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
        logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        return f"""# AI Research Analyst Report

**–¢–µ–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:** {research_topic}
**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** {timestamp}
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è–º

---

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞

–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ —Ö–æ–¥–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—É—á–∏–ª–∏ –±—ã –æ—Ü–µ–Ω–∫—É –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ {MIN_SCORE_THRESHOLD}.

**–í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π:** {len(self.all_papers_analyzed)}

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

1. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å —Ç–µ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
2. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–Ω–∏–∂–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞ –æ—Ü–µ–Ω–∫–∏
3. –î–æ–±–∞–≤—å—Ç–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Ç–µ—Ä–º–∏–Ω—ã

## –¢–æ–ø-5 —Å—Ç–∞—Ç–µ–π –ø–æ –æ—Ü–µ–Ω–∫–µ (–¥–∞–∂–µ –µ—Å–ª–∏ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞)

{self._get_top_analyzed_papers_summary()}
"""
    
    def _get_top_analyzed_papers_summary(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –ø–æ —Ç–æ–ø-5 –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å—Ç–∞—Ç—å—è–º"""
        if not self.all_papers_analyzed:
            return "–ù–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π."
        
        top_papers = sorted(self.all_papers_analyzed, key=lambda x: x.score, reverse=True)[:5]
        
        summary = []
        for i, paper in enumerate(top_papers, 1):
            summary.append(f"{i}. **{paper.title}** (–û—Ü–µ–Ω–∫–∞: {paper.score:.1f})")
        
        return "\n".join(summary)
    
    def _create_error_report(self, research_topic: str, error_message: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –æ—Ç—á–µ—Ç –æ–± –æ—à–∏–±–∫–µ"""
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        return f"""# AI Research Analyst Report - –û–®–ò–ë–ö–ê

**–¢–µ–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:** {research_topic}
**–î–∞—Ç–∞:** {timestamp}
**–°—Ç–∞—Ç—É—Å:** –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

---

## –û—à–∏–±–∫–∞

–ü—Ä–æ–∏–∑–æ—à–ª–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞:

```
{error_message}
```

**–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–æ –æ—à–∏–±–∫–∏:** {len(self.all_papers_analyzed)}
**–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π:** {len(self.validated_papers)}

–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–∏—Å—Ç–µ–º—ã –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.
"""
    
    def _print_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–æ–¥–∫—É"""
        logger.info("\n" + "="*60)
        logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê")
        logger.info("="*60)
        logger.info(f"‚úÖ –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(self.all_papers_analyzed)}")
        logger.info(f"üéØ –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(self.validated_papers)}")
        logger.info(f"üèÜ –¢–æ–ø –æ—Ü–µ–Ω–∫–∞: {self.validated_papers[0].score if self.validated_papers else 0}")
        logger.info(f"üìù –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {sum(p.score for p in self.validated_papers) / len(self.validated_papers) if self.validated_papers else 0:.2f}")
        logger.info("="*60)
    
    def get_results(self) -> Tuple[List[RankedPaper], List[RankedPaper]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—Å—Ç–∞—Ç—å–∏, –≤—Å–µ_–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—Å—Ç–∞—Ç—å–∏)
        """
        return self.validated_papers, self.all_papers_analyzed 