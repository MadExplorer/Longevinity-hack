#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä—ã workflow –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Extractor v2.0
"""

import json
import pathlib
from typing import List, Dict, Optional
from document_storage import DocumentStorage, DocumentProcessor
from pdf_reader import PDFReader
from extractor import KnowledgeExtractor
# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ config


class PDFProcessingPipeline:
    """
    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
    –•—Ä–∞–Ω–µ–Ω–∏–µ ‚Üí –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ‚Üí –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Üí –≠–∫—Å–ø–æ—Ä—Ç
    """
    
    def __init__(self, storage_path: str = "./pdf_documents"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞.
        
        Args:
            storage_path: –ü—É—Ç—å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è PDF —Ñ–∞–π–ª–æ–≤
        """
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.storage = DocumentStorage({
            'local_storage_path': storage_path
        })
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.processor = DocumentProcessor(self.storage)
        
        # –°–æ–∑–¥–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.text_extractor = KnowledgeExtractor()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.processing_results = []
    
    def add_pdf_from_url(self, pdf_url: str, source_id: str, 
                        metadata: Optional[Dict] = None) -> bool:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç PDF –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ URL.
        
        Args:
            pdf_url: URL PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞
            source_id: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            
        Returns:
            bool: True –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω
        """
        try:
            doc_ref = self.processor.add_pdf_for_processing(
                source=pdf_url,
                source_id=source_id,
                is_url=True,
                metadata=metadata or {}
            )
            
            print(f"‚úÖ PDF –¥–æ–±–∞–≤–ª–µ–Ω: {source_id} ({doc_ref.storage_type.value})")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è PDF {source_id}: {e}")
            return False
    
    def add_pdf_from_file(self, file_path: str, source_id: str, 
                         metadata: Optional[Dict] = None) -> bool:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç PDF –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
        
        Args:
            file_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            source_id: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            
        Returns:
            bool: True –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω
        """
        try:
            doc_ref = self.processor.add_pdf_for_processing(
                source=file_path,
                source_id=source_id,
                is_url=False,
                metadata=metadata or {}
            )
            
            print(f"‚úÖ PDF –¥–æ–±–∞–≤–ª–µ–Ω: {source_id} ({doc_ref.storage_type.value})")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è PDF {source_id}: {e}")
            return False
    
    def process_document(self, source_id: str) -> Dict:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ–ª–Ω—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º.
        
        Args:
            source_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        try:
            print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {source_id}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞—É—á–Ω—É—é –Ω–∞—Ä—Ä–∞—Ç–∏–≤—É
            result = self.processor.process_document_with_extractor(source_id)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_ref = self.storage.get_document(source_id)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            processing_result = {
                'source_id': source_id,
                'storage_info': {
                    'type': doc_ref.storage_type.value,
                    'location': doc_ref.location,
                    'size_mb': round((doc_ref.size_bytes or 0) / (1024 * 1024), 2),
                    'content_hash': doc_ref.content_hash
                } if doc_ref else None,
                'extraction_result': result.model_dump(),
                'summary': {
                    'total_statements': len(result.scientific_narrative),
                    'statement_types': self._get_statement_type_counts(result.scientific_narrative),
                    'total_triples': sum(len(stmt.knowledge_triples) for stmt in result.scientific_narrative)
                }
            }
            
            self.processing_results.append(processing_result)
            
            print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(result.scientific_narrative)} —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π")
            return processing_result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {source_id}: {e}")
            return {'source_id': source_id, 'error': str(e)}
    
    def _get_statement_type_counts(self, statements) -> Dict[str, int]:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –ø–æ —Ç–∏–ø–∞–º."""
        counts = {}
        for stmt in statements:
            stmt_type = stmt.statement_type
            counts[stmt_type] = counts.get(stmt_type, 0) + 1
        return counts
    
    def process_all_documents(self) -> List[Dict]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        
        Returns:
            List[Dict]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        all_docs = self.storage.list_documents()
        
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞–∫–µ—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(all_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        results = []
        for doc_ref in all_docs:
            result = self.process_document(doc_ref.source_id)
            results.append(result)
        
        print(f"üèÅ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return results
    
    def export_results(self, output_file: str = "extraction_results.jsonl") -> str:
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ JSONL —Ñ–∞–π–ª.
        
        Args:
            output_file: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            
        Returns:
            str: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        output_path = pathlib.Path(output_file)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for result in self.processing_results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {output_path.absolute()}")
        return str(output_path.absolute())
    
    def get_pipeline_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–π–ø–ª–∞–π–Ω–∞."""
        storage_stats = self.storage.get_storage_stats()
        
        total_statements = sum(
            result.get('summary', {}).get('total_statements', 0) 
            for result in self.processing_results
        )
        
        total_triples = sum(
            result.get('summary', {}).get('total_triples', 0) 
            for result in self.processing_results
        )
        
        return {
            'storage': storage_stats,
            'processing': {
                'processed_documents': len(self.processing_results),
                'total_statements': total_statements,
                'total_knowledge_triples': total_triples
            }
        }


def workflow_example_1_research_papers():
    """
    Workflow 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –æ –¥–æ–ª–≥–æ–ª–µ—Ç–∏–∏
    """
    print("üìö Workflow 1: –ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –æ –¥–æ–ª–≥–æ–ª–µ—Ç–∏–∏")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = PDFProcessingPipeline("./longevity_papers")
    
    # –ü—Ä–∏–º–µ—Ä—ã –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π (—Ä–µ–∞–ª—å–Ω—ã–µ URL)
    papers = [
        {
            'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3654510/pdf/nihms-462482.pdf',
            'id': 'caloric_restriction_2013',
            'metadata': {
                'title': 'Caloric Restriction and Human Longevity',
                'topic': 'caloric_restriction',
                'year': 2013,
                'source': 'PMC'
            }
        }
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    for paper in papers:
        pipeline.add_pdf_from_url(
            pdf_url=paper['url'],
            source_id=paper['id'],
            metadata=paper['metadata']
        )
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    results = pipeline.process_all_documents()
    
    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    pipeline.export_results("longevity_papers_results.jsonl")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = pipeline.get_pipeline_stats()
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(json.dumps(stats, indent=2, ensure_ascii=False))


def workflow_example_2_local_files():
    """
    Workflow 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö PDF —Ñ–∞–π–ª–æ–≤
    """
    print("üìÅ Workflow 2: –õ–æ–∫–∞–ª—å–Ω—ã–µ PDF —Ñ–∞–π–ª—ã")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = PDFProcessingPipeline("./local_pdfs")
    
    # –ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    local_files = [
        {
            'path': '/path/to/aging_research.pdf',
            'id': 'aging_001',
            'metadata': {'topic': 'aging_mechanisms'}
        }
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
    for file_info in local_files:
        if pathlib.Path(file_info['path']).exists():
            pipeline.add_pdf_from_file(
                file_path=file_info['path'],
                source_id=file_info['id'],
                metadata=file_info['metadata']
            )
        else:
            print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_info['path']}")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    pipeline.process_all_documents()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = pipeline.get_pipeline_stats()
    print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['processing']['processed_documents']}")


def workflow_example_3_batch_processing():
    """
    Workflow 3: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    """
    print("‚ö° Workflow 3: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    config = {
        'documents': [
            {
                'source': 'https://example.com/paper1.pdf',
                'source_id': 'longevity_paper_1',
                'is_url': True,
                'metadata': {'topic': 'mTOR', 'year': 2024}
            },
            {
                'source': 'https://example.com/paper2.pdf',
                'source_id': 'longevity_paper_2',
                'is_url': True,
                'metadata': {'topic': 'autophagy', 'year': 2024}
            }
        ]
    }
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = PDFProcessingPipeline("./batch_processed")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    for doc_config in config['documents']:
        if doc_config['is_url']:
            pipeline.add_pdf_from_url(
                pdf_url=doc_config['source'],
                source_id=doc_config['source_id'],
                metadata=doc_config.get('metadata', {})
            )
        else:
            pipeline.add_pdf_from_file(
                file_path=doc_config['source'],
                source_id=doc_config['source_id'],
                metadata=doc_config.get('metadata', {})
            )
    
    # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    results = pipeline.process_all_documents()
    
    # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_file = pipeline.export_results("batch_results.jsonl")
    
    print(f"‚úÖ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_file}")


def main():
    """–ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ workflow."""
    print("üîÑ –ü—Ä–∏–º–µ—Ä—ã Workflow –¥–ª—è PDF Processing Pipeline")
    print("=" * 60)
    
    # –í—ã–±–∏—Ä–∞–µ–º workflow
    workflows = [
        ("1", "–ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –æ –¥–æ–ª–≥–æ–ª–µ—Ç–∏–∏", workflow_example_1_research_papers),
        ("2", "–õ–æ–∫–∞–ª—å–Ω—ã–µ PDF —Ñ–∞–π–ª—ã", workflow_example_2_local_files),
        ("3", "–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞", workflow_example_3_batch_processing)
    ]
    
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ workflow:")
    for num, description, _ in workflows:
        print(f"  {num}. {description}")
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∑–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–µ—Ç–∏–π workflow (–¥–µ–º–æ)
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫–∞–µ–º Workflow 3 (–¥–µ–º–æ)...")
    workflow_example_3_batch_processing()


if __name__ == "__main__":
    main() 