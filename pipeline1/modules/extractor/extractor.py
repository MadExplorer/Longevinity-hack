#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å 2: Extractor - –ê–≥–µ–Ω—Ç-–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –ó–Ω–∞–Ω–∏–π
–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏–∑ –Ω–∞—É—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM
"""

import os
import json
import hashlib
import pathlib
from typing import List, Optional, Dict, Any
import asyncio
from openai import OpenAI
import instructor
from tqdm import tqdm
import jsonlines
from models import ExtractedDocument, InputDocument, RESEARCH_AREAS, ENTITY_TYPES
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from config.config import ExtractorConfig


class KnowledgeExtractor:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏–∑ –Ω–∞—É—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è:
    1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –æ–±–ª–∞—Å—Ç–∏ –∏ –∑—Ä–µ–ª–æ—Å—Ç–∏
    2. –ò–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
    """
    
    def __init__(self, config: Optional[ExtractorConfig] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
        """
        self.config = config or ExtractorConfig()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
        if not self.config.llm_api_key:
            raise ValueError(
                "API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OPENAI_API_KEY –∏–ª–∏ GEMINI_API_KEY"
            )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LLM –∫–ª–∏–µ–Ω—Ç
        if self.config.llm_base_url:
            openai_client = OpenAI(api_key=self.config.llm_api_key, base_url=self.config.llm_base_url)
        else:
            openai_client = OpenAI(api_key=self.config.llm_api_key)
        
        self.llm_client = instructor.from_openai(openai_client)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç
        self.master_prompt = self._load_prompt()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "processed": 0,
            "cached": 0,
            "errors": 0,
            "total_entities": 0,
            "total_relationships": 0
        }
    
    def _load_prompt(self) -> str:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞—Å—Ç–µ—Ä-–ø—Ä–æ–º–ø—Ç –∏–∑ —Ñ–∞–π–ª–∞."""
        try:
            with open(self.config.prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            # Fallback –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω
            return """
            –¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–±–∏–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞—É—á–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∏:
            1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é (research_area, maturity_level)
            2. –°—É—â–Ω–æ—Å—Ç–∏ –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –¥–ª—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
            
            –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ JSON –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.
            """
    
    def _generate_cache_key(self, document: InputDocument) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫—ç—à–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        content = f"{document.title}||{document.abstract}"
        return hashlib.sha256(content.encode()).hexdigest()
    
    def _get_cached_result(self, cache_key: str) -> Optional[ExtractedDocument]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞."""
        if not self.config.cache_enabled:
            return None
        
        cache_file = self.config.cache_dir / f"{cache_key}.json"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return ExtractedDocument(**data)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞ {cache_key}: {e}")
        
        return None
    
    def _save_to_cache(self, cache_key: str, result: ExtractedDocument):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à."""
        if not self.config.cache_enabled:
            return
        
        cache_file = self.config.cache_dir / f"{cache_key}.json"
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(result.model_dump(), f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫—ç—à {cache_key}: {e}")
    
    def extract_knowledge(self, document: InputDocument) -> Optional[ExtractedDocument]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –∏–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        
        Args:
            document: –í—Ö–æ–¥–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
            
        Returns:
            ExtractedDocument: –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = self._generate_cache_key(document)
        cached_result = self._get_cached_result(cache_key)
        
        if cached_result:
            self.stats["cached"] += 1
            return cached_result
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        text_to_analyze = f"Title: {document.title}\nAbstract: {document.abstract}"
        if document.content:
            text_to_analyze += f"\nContent: {document.content[:1000]}..."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        full_prompt = f"{self.master_prompt}\n\n–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{text_to_analyze}"
        
        try:
            # –í—ã–∑—ã–≤–∞–µ–º LLM —Å –ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏–µ–º –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            result = self.llm_client.chat.completions.create(
                model=self.config.llm_model,
                response_model=ExtractedDocument,
                messages=[
                    {
                        "role": "system", 
                        "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∑–Ω–∞–Ω–∏–π –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤."
                    },
                    {
                        "role": "user", 
                        "content": full_prompt
                    }
                ],
                temperature=0.1,
                max_retries=2
            )
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º source_id –∏ source_url –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            result.source_id = document.source_id
            result.source_url = document.source_url
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self._save_to_cache(cache_key, result)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats["processed"] += 1
            self.stats["total_entities"] += len(result.knowledge_graph.entities)
            self.stats["total_relationships"] += len(result.knowledge_graph.relationships)
            
            return result
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document.source_id}: {e}")
            self.stats["errors"] += 1
            return None
    
    async def extract_knowledge_batch(self, documents: List[InputDocument]) -> List[Optional[ExtractedDocument]]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            List[Optional[ExtractedDocument]]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        async def process_single(doc: InputDocument) -> Optional[ExtractedDocument]:
            async with semaphore:
                # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self.extract_knowledge, doc)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [process_single(doc) for doc in documents]
        return await asyncio.gather(*tasks, return_exceptions=False)
    
    def process_jsonl_file(self, input_file: str, output_file: str):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç JSONL —Ñ–∞–π–ª —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.
        
        Args:
            input_file: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        print(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {input_file}")
        
        # –ß–∏—Ç–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = []
        try:
            with jsonlines.open(input_file, 'r') as reader:
                for line in reader:
                    try:
                        doc = InputDocument(**line)
                        documents.append(doc)
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏: {e}")
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_file}")
            return
        
        if not documents:
            print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç–∞–º–∏
        results = []
        
        with tqdm(total=len(documents), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤") as pbar:
            for i in range(0, len(documents), self.config.batch_size):
                batch = documents[i:i + self.config.batch_size]
                
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    batch_results = loop.run_until_complete(
                        self.extract_knowledge_batch(batch)
                    )
                    results.extend(batch_results)
                finally:
                    loop.close()
                
                pbar.update(len(batch))
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        successful_results = [r for r in results if r is not None]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        try:
            with jsonlines.open(output_file, 'w') as writer:
                for result in successful_results:
                    writer.write(result.model_dump())
            
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._print_stats(len(documents), len(successful_results))
    
    def _print_stats(self, total_input: int, total_output: int):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        print(f"  –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {total_input}")
        print(f"  –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['processed']}")
        print(f"  –í–∑—è—Ç–æ –∏–∑ –∫—ç—à–∞: {self.stats['cached']}")
        print(f"  –û—à–∏–±–æ–∫: {self.stats['errors']}")
        print(f"  –ò—Ç–æ–≥–æ –Ω–∞ –≤—ã—Ö–æ–¥–µ: {total_output}")
        print(f"  –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {self.stats['total_entities']}")
        print(f"  –ò–∑–≤–ª–µ—á–µ–Ω–æ –æ—Ç–Ω–æ—à–µ–Ω–∏–π: {self.stats['total_relationships']}")


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è CLI."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Knowledge Extractor - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∏–∑ –Ω–∞—É—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    parser.add_argument("--input", "-i", help="–í—Ö–æ–¥–Ω–æ–π JSONL —Ñ–∞–π–ª", default="harvested_data.jsonl")
    parser.add_argument("--output", "-o", help="–í—ã—Ö–æ–¥–Ω–æ–π JSONL —Ñ–∞–π–ª", default="extracted_data.jsonl")
    parser.add_argument("--batch-size", "-b", type=int, help="–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞", default=5)
    parser.add_argument("--no-cache", action="store_true", help="–û—Ç–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ")
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = ExtractorConfig()
    config.input_filename = args.input
    config.output_filename = args.output
    config.batch_size = args.batch_size
    config.cache_enabled = not args.no_cache
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    try:
        extractor = KnowledgeExtractor(config)
        extractor.process_jsonl_file(args.input, args.output)
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main() 