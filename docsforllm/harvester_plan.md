Техническое Задание: Модуль "Harvester" (Сборщик Данных)
1. Цель и Философия
Цель: Создать автономный, отказоустойчивый и расширяемый модуль, который является "органом чувств" нашей системы в научном мире. Его задача — по высокоуровневой теме находить, скачивать, преобразовывать и унифицировать релевантную научную литературу из различных источников.
Философия:
"Не доверяй, а проверяй": Каждый шаг (поиск, скачивание, парсинг) должен логироваться и обрабатывать ошибки.
"Не повторяйся": Агрессивное кэширование на всех этапах (скачанные файлы, извлеченный текст) для экономии времени и денег.
"Говори на одном языке": Все данные на выходе, независимо от источника, должны быть приведены к единой, строгой модели данных.
2. Ключевые Компоненты
Модуль "Harvester" состоит из трех логических компонентов, которые работают последовательно:
Query Strategist (Стратег Запросов): "Мозг", который решает, ЧТО искать.
Data Fetchers (Сборщики Данных): "Руки", которые БЕРУТ данные из разных источников (PubMed, arXiv и т.д.).
Data Processor (Обработчик Данных): "Пищеварительная система", которая ПРИВОДИТ В ПОРЯДОК сырые данные.
3. Архитектура и Взаимодействие
Generated code
[Высокоуровневая Тема]
       |
       v
[1. Query Strategist] -> (генерирует список запросов)
       |
       v
[2. Data Fetchers]  -> (для каждого запроса, скачивает сырые данные: XML, PDF)
    - PubMed Fetcher
    - arXiv Fetcher
    - ...
       |
       v
[3. Data Processor] -> (обрабатывает сырые данные, извлекает текст)
    - PDF Reader (Gemini VLM)
    - Cache Manager
       |
       v
[Унифицированный Корпус Данных (JSON)] -> (выход для Модуля 2, Extractor)
Use code with caution.
4. Детальное ТЗ по Компонентам
4.1. Query Strategist
Задача: Превратить одну общую тему в несколько точных, диверсифицированных поисковых запросов.
Вход: initial_topic: str.
Логика:
Использовать PROMPT_QUERY_GENERATOR (из наших предыдущих обсуждений).
Вызвать LLM (Gemini/GPT) для генерации списка из 5-7 запросов.
Выход: List[str] — список поисковых строк.
4.2. Data Fetchers
Это должен быть "плагинный" компонент. Вы легко сможете добавлять новые источники.
pubmed_fetcher.py
Задача: Поиск и скачивание метаданных и абстрактов из PubMed.
Вход: query: str, start_date: str, end_date: str.
Инструменты: Библиотека biopython.
Логика:
Использовать Entrez.esearch с параметрами term, mindate, maxdate для получения списка PMID.
Использовать Entrez.efetch для получения полных записей по списку PMID.
Выход: Словарь с сырыми, не обработанными данными.
arxiv_harvester.py (Опционально, но рекомендуется)
Задача: Поиск и скачивание PDF-файлов с arXiv.
Вход: query: str.
Инструменты: Официальная библиотека arxiv.
Логика:
Использовать arxiv.Search для поиска статей.
Для каждого результата, используя result.download_pdf(), скачать PDF в локальную папку (например, downloaded_pdfs/).
Выход: Список путей к скачанным PDF-файлам.
4.3. Data Processor
Это самый важный компонент для обеспечения качества данных.
pdf_reader.py (Используя Gemini)
Задача: Превратить "мертвый" PDF в "живой" текст.
Вход: pdf_path: str.
Инструменты: google-genai (как в вашем коде).
Логика:
Прочитать PDF как байты.
Отправить в мультимодальную модель Gemini с промптом, требующим извлечь полный текст, а не саммари.
Выход: str — полный текст статьи.
cache_manager.py
Задача: Кэшировать результаты тяжелых операций.
Логика:
Кэширование PDF-текста: Перед тем как вызывать pdf_reader, проверить, нет ли уже извлеченного текста для этого файла в cache/pdf_texts.json.
Кэширование Результатов Извлечения: (Для Модуля 2) Сохранять JSON-вывод от Extractor-а, чтобы не извлекать концепты из одной и той же статьи дважды.
Главный процессор
Задача: Взять сырые данные от всех Fetcher-ов и превратить их в единый формат.
Вход: Сырые данные (XML от PubMed, пути к PDF от arXiv).
Логика:
Для записей PubMed: извлечь PMID, title, abstract, year. Поле full_text оставить пустым.
Для записей arXiv:
Извлечь ID, title, summary (abstract), year.
Вызвать pdf_reader (с проверкой кэша), чтобы получить full_text.
Выход (Финальный Контракт): Единый corpus.json, представляющий собой словарь словарей:
Generated json
{
  "PMID:12345": {
    "title": "...",
    "year": 2019,
    "abstract": "...",
    "full_text": "", 
    "source": "pubmed"
  },
  "arXiv:2408.06292": {
    "title": "The AI Scientist...",
    "year": 2024,
    "abstract": "...",
    "full_text": "Полный текст, извлеченный из PDF...",
    "source": "arxiv"
  }
}
Use code with caution.
Json
5. Главный Пайплайн Харвестера (harvester_orchestrator.py)
Это главный скрипт, который вы будете запускать.
Generated python
def run_harvesting_pipeline(topic: str, start_date: str, end_date: str):
    # 1. Сгенерировать запросы
    strategist = QueryStrategist()
    queries = strategist.generate(topic)

    # 2. Скачать данные
    pubmed_fetcher = PubMedFetcher()
    # arxiv_fetcher = ArXivFetcher() # (если реализован)

    raw_pubmed_data = pubmed_fetcher.fetch(queries, start_date, end_date)
    # raw_arxiv_pdfs = arxiv_fetcher.fetch(queries)

    # 3. Обработать и унифицировать данные
    processor = DataProcessor()
    unified_corpus = processor.process(raw_pubmed_data) #, raw_arxiv_pdfs

    # 4. Сохранить финальный корпус
    with open("final_corpus.json", "w") as f:
        json.dump(unified_corpus, f)
        
    return unified_corpus
Use code with caution.
Python
Этот детальный план дает вам четкую, профессиональную и масштабируемую архитектуру для первого и самого важного модуля вашей системы.