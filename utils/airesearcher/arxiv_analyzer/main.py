"""
–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ arXiv —Å—Ç–∞—Ç–µ–π –ø–æ —á–µ–∫–ª–∏—Å—Ç—É –∏–∑ initialtask.md
"""

import asyncio
import json
import time
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

try:
    from .query_generator import QueryGenerator
    from .arxiv_client import ArxivClient
    from .paper_analyzer import PaperAnalyzer
    from .priority_ranker import PriorityRanker
    from .state_manager import StateManager
    from .models import ArxivQuery, PaperInfo, PaperAnalysis, RankedPaper
    from .config import DEFAULT_MAX_RESULTS
except ImportError:
    # Fallback –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
    from query_generator import QueryGenerator
    from arxiv_client import ArxivClient
    from paper_analyzer import PaperAnalyzer
    from priority_ranker import PriorityRanker
    from state_manager import StateManager
    from models import ArxivQuery, PaperInfo, PaperAnalysis, RankedPaper
    from config import DEFAULT_MAX_RESULTS

# –ò–º–ø–æ—Ä—Ç data_loader –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PDF —Ñ–∞–π–ª–∞–º–∏
try:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent.parent / "lcgr"))
    from data_loader import load_documents
except ImportError:
    print("‚ö†Ô∏è –ú–æ–¥—É–ª—å data_loader –Ω–µ –Ω–∞–π–¥–µ–Ω. –ê–Ω–∞–ª–∏–∑ PDF —Ñ–∞–π–ª–æ–≤ –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
    load_documents = None


class ArxivAnalyzer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ç–µ–π arXiv —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
    
    def __init__(self, enable_state_tracking: bool = True, custom_output_dir: Optional[str] = None, pdf_directory: Optional[str] = None):
        self.query_generator = QueryGenerator()
        self.paper_analyzer = PaperAnalyzer()
        self.priority_ranker = PriorityRanker()
        
        # –ü–∞–ø–∫–∞ —Å PDF —Ñ–∞–π–ª–∞–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.pdf_directory = pdf_directory or "lcgr/downloaded_pdfs/references_dlya_statiy_2025"
        
        # –ú–µ–Ω–µ–¥–∂–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –ø—É—Ç–µ–π
        self.enable_state_tracking = enable_state_tracking
        self.custom_output_dir = custom_output_dir
        
        if enable_state_tracking:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—É—Ç–µ–π –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
            try:
                from .config import get_output_paths, create_output_structure
            except ImportError:
                from config import get_output_paths, create_output_structure
            
            if custom_output_dir:
                paths = get_output_paths(custom_output_dir)
            else:
                paths = get_output_paths()
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–∞–ª–æ–≥–æ–≤
            create_output_structure(paths["base"])
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º StateManager —Å –Ω–æ–≤—ã–º –ø—É—Ç–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è
            self.state_manager = StateManager(str(paths["state"]))
            print(f"üèóÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {paths['state']}")
        else:
            self.state_manager = None
    
    async def run_full_analysis(
        self, 
        max_papers_per_query: int = DEFAULT_MAX_RESULTS,
        max_total_papers: int = 50,
        use_llm_ranking: bool = True,
        incremental: bool = True
    ) -> Dict:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ ‚Üí –ø–æ–∏—Å–∫ ‚Üí –∞–Ω–∞–ª–∏–∑ ‚Üí —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
        
        Args:
            max_papers_per_query: –ú–∞–∫—Å–∏–º—É–º —Å—Ç–∞—Ç–µ–π –Ω–∞ –∑–∞–ø—Ä–æ—Å
            max_total_papers: –ú–∞–∫—Å–∏–º—É–º —Å—Ç–∞—Ç–µ–π –≤—Å–µ–≥–æ  
            use_llm_ranking: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            incremental: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º (–ø—Ä–æ–ø—É—Å–∫–∞—Ç—å —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        """
        
        start_time = time.time()
        print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ arXiv —Å—Ç–∞—Ç–µ–π...")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        if self.enable_state_tracking and self.state_manager:
            self.state_manager.print_progress_summary()
        
        # –≠—Ç–∞–ø 0: –ó–∞–≥—Ä—É–∑–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è –∑–∞–¥–∞—á–∏ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤  
        task_description = self.query_generator.load_task_description()
        task_hash = None
        if self.enable_state_tracking and self.state_manager:
            task_hash = self.state_manager.get_task_hash(task_description)
        
        # –≠—Ç–∞–ø 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        print("\nüìù –≠—Ç–∞–ø 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            queries = None
            if self.enable_state_tracking and self.state_manager and task_hash:
                queries = self.state_manager.get_cached_queries(task_hash)
                if queries:
                    print(f"‚ôªÔ∏è  –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(queries)} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
            
            # –ï—Å–ª–∏ –∫—ç—à–∞ –Ω–µ—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            if not queries:
                queries = await self.query_generator.generate_queries(max_papers_per_query)
                print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(queries)} –Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
                
                # –ö—ç—à–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã
                if self.enable_state_tracking and self.state_manager and task_hash:
                    self.state_manager.cache_queries(task_hash, queries)
                    print("üíæ –ó–∞–ø—Ä–æ—Å—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à")
            
            for i, query in enumerate(queries, 1):
                print(f"   {i}. {query.strategy.value}: {query.query}")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤: {e}"}
        
        # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        session_id = None
        if self.enable_state_tracking and self.state_manager:
            session_id = self.state_manager.start_session(task_description, queries)
            print(f"üìã –ù–∞—á–∞—Ç–∞ —Å–µ—Å—Å–∏—è: {session_id}")
        
        # –≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ arXiv
        print("\nüîç –≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ arXiv...")
        try:
            async with ArxivClient() as client:
                search_results = await client.search_multiple_queries(queries)
                
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                all_papers = []
                for strategy, result in search_results.items():
                    if 'papers' in result:
                        all_papers.extend(result['papers'])
                        print(f"   {strategy}: –Ω–∞–π–¥–µ–Ω–æ {result['count']} —Å—Ç–∞—Ç–µ–π")
                
                unique_papers = client.filter_duplicates(all_papers)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
                if incremental and self.enable_state_tracking and self.state_manager:
                    new_papers = self.state_manager.filter_new_papers(unique_papers)
                    already_analyzed = len(unique_papers) - len(new_papers)
                    if already_analyzed > 0:
                        print(f"‚ôªÔ∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ {already_analyzed} —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
                        unique_papers = new_papers
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π
                if len(unique_papers) > max_total_papers:
                    unique_papers = unique_papers[:max_total_papers]
                    print(f"   ‚ö†Ô∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {max_total_papers} —Å—Ç–∞—Ç–µ–π")
                
                print(f"‚úÖ –ö –∞–Ω–∞–ª–∏–∑—É: {len(unique_papers)} —Å—Ç–∞—Ç–µ–π")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π: {e}"}
        
        if not unique_papers:
            if incremental and self.enable_state_tracking:
                print("‚ÑπÔ∏è  –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                summary = self.state_manager.get_progress_summary()
                return {"message": "–í—Å–µ —Å—Ç–∞—Ç—å–∏ —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã", "progress_summary": summary}
            else:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return {"error": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
        
        # –≠—Ç–∞–ø 3: –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π –ø–æ —á–µ–∫–ª–∏—Å—Ç—É
        print(f"\nüß† –≠—Ç–∞–ø 3: –ê–Ω–∞–ª–∏–∑ {len(unique_papers)} —Å—Ç–∞—Ç–µ–π –ø–æ —á–µ–∫–ª–∏—Å—Ç—É...")
        try:
            analyses = await self.paper_analyzer.analyze_papers_batch(
                unique_papers, 
                max_concurrent=3
            )
            print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(analyses)} —Å—Ç–∞—Ç–µ–π")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            if self.enable_state_tracking and self.state_manager and session_id:
                for analysis in analyses:
                    self.state_manager.save_paper_analysis(analysis, session_id)
                print("üíæ –ê–Ω–∞–ª–∏–∑—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            valid_analyses = [a for a in analyses if a.overall_score > 0.1]
            avg_score = sum(a.overall_score for a in valid_analyses) / len(valid_analyses) if valid_analyses else 0
            print(f"   üìä –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {avg_score:.2f}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ç–µ–π: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ç–µ–π: {e}"}
        
        # –≠—Ç–∞–ø 4: –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏
        print("\nüìä –≠—Ç–∞–ø 4: –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏...")
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º, –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∞–Ω–∞–ª–∏–∑–∞–º–∏
        all_analyses = analyses
        if incremental and self.enable_state_tracking and self.state_manager:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–∞–ª–∏–∑—ã –∏ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å –Ω–æ–≤—ã–º–∏
            # (–¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è)
            print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è...")
            
            # –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–ø—Ä–æ—â–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
            try:
                from .models import PaperInfo, AnalysisScore, PaperAnalysis
                from .models import PrioritizationAnalysis, ValidationAnalysis, ArchitectureAnalysis
                from .models import KnowledgeAnalysis, ImplementationAnalysis
            except ImportError:
                from models import PaperInfo, AnalysisScore, PaperAnalysis
                from models import PrioritizationAnalysis, ValidationAnalysis, ArchitectureAnalysis
                from models import KnowledgeAnalysis, ImplementationAnalysis
            
            all_analyzed_papers = []
            for arxiv_id, paper_state in self.state_manager.analyzed_papers.items():
                
                # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
                dummy_score = AnalysisScore(score=3, explanation="–ò–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è")
                dummy_analysis = PaperAnalysis(
                    paper_info=PaperInfo(
                        title=paper_state.title,
                        authors=[],
                        abstract="",
                        arxiv_id=paper_state.arxiv_id,
                        pdf_url="",
                        published="",
                        categories=[]
                    ),
                    prioritization=PrioritizationAnalysis(
                        algorithm_search=dummy_score,
                        relevance_justification=dummy_score,
                        knowledge_gaps=dummy_score,
                        balance_hotness_novelty=dummy_score
                    ),
                    validation=ValidationAnalysis(
                        benchmarks=dummy_score,
                        metrics=dummy_score,
                        evaluation_methodology=dummy_score,
                        expert_validation=dummy_score
                    ),
                    architecture=ArchitectureAnalysis(
                        roles_and_sops=dummy_score,
                        communication=dummy_score,
                        memory_context=dummy_score,
                        self_correction=dummy_score
                    ),
                    knowledge=KnowledgeAnalysis(
                        extraction=dummy_score,
                        representation=dummy_score,
                        conflict_resolution=dummy_score
                    ),
                    implementation=ImplementationAnalysis(
                        tools_frameworks=dummy_score,
                        open_source=dummy_score,
                        reproducibility=dummy_score
                    ),
                    overall_score=paper_state.overall_score,
                    key_insights=["–ò–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"],
                    relevance_to_task="–†–∞–Ω–µ–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è"
                )
                all_analyzed_papers.append(dummy_analysis)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏ –Ω–æ–≤—ã–µ –∞–Ω–∞–ª–∏–∑—ã
            all_analyses = all_analyzed_papers + analyses
            print(f"üìä –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: {len(all_analyses)} —Å—Ç–∞—Ç–µ–π")
        
        try:
            if use_llm_ranking:
                ranked_papers = await self.priority_ranker.rank_papers_with_llm(all_analyses)
                print("‚úÖ –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å LLM –∞–Ω–∞–ª–∏–∑–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            else:
                ranked_papers = self.priority_ranker.rank_papers_simple(all_analyses)
                print("‚úÖ –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            if self.enable_state_tracking and self.state_manager and session_id:
                self.state_manager.save_ranking_session(ranked_papers, session_id)
                print("üíæ –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            summary = self.priority_ranker.get_ranking_summary(ranked_papers)
            print(f"   üèÜ –¢–æ–ø —Å—Ç–∞—Ç—å—è: {summary['top_paper']['title'][:50]}..." if summary['top_paper'] else "")
            print(f"   üìà –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–ø-5: {summary['top_5_avg_score']:.2f}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}"}
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏
        if self.enable_state_tracking and self.state_manager and session_id:
            self.state_manager.complete_session(session_id, len(unique_papers))
            print(f"‚úÖ –°–µ—Å—Å–∏—è {session_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        end_time = time.time()
        duration = end_time - start_time
        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.1f} —Å–µ–∫—É–Ω–¥")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "timestamp": datetime.now().isoformat(),
            "session_id": session_id,
            "duration_seconds": duration,
            "incremental_mode": incremental,
            "statistics": {
                "queries_generated": len(queries),
                "papers_found": len(unique_papers),
                "papers_analyzed": len(analyses),
                "total_papers_in_ranking": len(all_analyses),
                "valid_analyses": len([a for a in analyses if a.overall_score > 0.1])
            },
            "queries": [{"strategy": q.strategy.value, "query": q.query} for q in queries],
            "ranking_summary": summary,
            "top_papers": self._format_top_papers(ranked_papers[:10]),
            "full_results": ranked_papers  # –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        }
        
        return result
    
    async def run_pdf_analysis(
        self,
        max_papers: int = 50,
        use_llm_ranking: bool = True,
        use_cache: bool = True,
        max_workers: int = 4
    ) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç PDF —Ñ–∞–π–ª—ã –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–∏
        
        Args:
            max_papers: –ú–∞–∫—Å–∏–º—É–º —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            use_llm_ranking: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è  
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à –¥–ª—è PDF —Ñ–∞–π–ª–æ–≤
            max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF
        """
        
        if load_documents is None:
            return {"error": "–ú–æ–¥—É–ª—å data_loader –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω"}
        
        start_time = time.time()
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ PDF —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏: {self.pdf_directory}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        if self.enable_state_tracking and self.state_manager:
            self.state_manager.print_progress_summary()
        
        # –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ PDF —Ñ–∞–π–ª–æ–≤
        print(f"\nüìÅ –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ PDF —Ñ–∞–π–ª–æ–≤ –∏–∑ {self.pdf_directory}...")
        try:
            documents = load_documents(
                data_source=self.pdf_directory,
                use_cache=use_cache,
                max_workers=max_workers
            )
            
            if not documents:
                return {"error": f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ PDF —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ {self.pdf_directory}"}
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if len(documents) > max_papers:
                documents = dict(list(documents.items())[:max_papers])
                print(f"   ‚ö†Ô∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {max_papers} —Ñ–∞–π–ª–æ–≤")
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} PDF —Ñ–∞–π–ª–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PDF —Ñ–∞–π–ª–æ–≤: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PDF —Ñ–∞–π–ª–æ–≤: {e}"}
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç PaperInfo –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        papers = []
        for paper_id, doc_data in documents.items():
            # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞—Ç—å–µ –∏–∑ PDF
            paper_info = PaperInfo(
                title=f"PDF Document: {paper_id}",
                authors=[],
                abstract=doc_data["full_text"][:1000],  # –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
                arxiv_id=paper_id,
                pdf_url="",
                published=str(doc_data.get("year", 2024)),
                categories=[]
            )
            papers.append(paper_info)
        
        # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
        session_id = None
        if self.enable_state_tracking and self.state_manager:
            session_id = self.state_manager.start_session(f"PDF Analysis: {self.pdf_directory}", [])
            print(f"üìã –ù–∞—á–∞—Ç–∞ —Å–µ—Å—Å–∏—è: {session_id}")
        
        # –≠—Ç–∞–ø 2: –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —á–µ–∫–ª–∏—Å—Ç—É
        print(f"\nüß† –≠—Ç–∞–ø 2: –ê–Ω–∞–ª–∏–∑ {len(papers)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —á–µ–∫–ª–∏—Å—Ç—É...")
        try:
            # –ó–∞–º–µ–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç–µ–π –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –∏–∑ PDF
            for i, paper in enumerate(papers):
                paper_id = paper.arxiv_id
                if paper_id in documents:
                    # –ó–∞–º–µ–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
                    paper.abstract = documents[paper_id]["full_text"]
            
            analyses = await self.paper_analyzer.analyze_papers_batch(
                papers, 
                max_concurrent=3
            )
            print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(analyses)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            if self.enable_state_tracking and self.state_manager and session_id:
                for analysis in analyses:
                    self.state_manager.save_paper_analysis(analysis, session_id)
                print("üíæ –ê–Ω–∞–ª–∏–∑—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            valid_analyses = [a for a in analyses if a.overall_score > 0.1]
            avg_score = sum(a.overall_score for a in valid_analyses) / len(valid_analyses) if valid_analyses else 0
            print(f"   üìä –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {avg_score:.2f}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}"}
        
        # –≠—Ç–∞–ø 3: –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏
        print("\nüìä –≠—Ç–∞–ø 3: –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏...")
        try:
            if use_llm_ranking:
                ranked_papers = await self.priority_ranker.rank_papers_with_llm(analyses)
                print("‚úÖ –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å LLM –∞–Ω–∞–ª–∏–∑–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            else:
                ranked_papers = self.priority_ranker.rank_papers_simple(analyses)
                print("‚úÖ –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            if self.enable_state_tracking and self.state_manager and session_id:
                self.state_manager.save_ranking_session(ranked_papers, session_id)
                print("üíæ –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            summary = self.priority_ranker.get_ranking_summary(ranked_papers)
            print(f"   üèÜ –¢–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç: {summary['top_paper']['title'][:50]}..." if summary['top_paper'] else "")
            print(f"   üìà –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–ø-5: {summary['top_5_avg_score']:.2f}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}"}
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏
        if self.enable_state_tracking and self.state_manager and session_id:
            self.state_manager.complete_session(session_id, len(papers))
            print(f"‚úÖ –°–µ—Å—Å–∏—è {session_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        end_time = time.time()
        duration = end_time - start_time
        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ PDF —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.1f} —Å–µ–∫—É–Ω–¥")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "timestamp": datetime.now().isoformat(),
            "session_id": session_id,
            "duration_seconds": duration,
            "analysis_type": "pdf_analysis",
            "pdf_directory": self.pdf_directory,
            "statistics": {
                "pdf_files_loaded": len(documents),
                "papers_analyzed": len(analyses),
                "valid_analyses": len([a for a in analyses if a.overall_score > 0.1])
            },
            "ranking_summary": summary,
            "top_papers": self._format_top_papers(ranked_papers[:10]),
            "full_results": ranked_papers
        }
        
        return result
    
    def show_progress(self) -> Dict:
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –∞–Ω–∞–ª–∏–∑–∞"""
        if not self.enable_state_tracking or not self.state_manager:
            return {"error": "–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–æ"}
        
        return self.state_manager.get_progress_summary()
    
    def print_progress(self):
        """–í—ã–≤–æ–¥–∏—Ç —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –∞–Ω–∞–ª–∏–∑–∞"""
        if not self.enable_state_tracking or not self.state_manager:
            print("‚ö†Ô∏è –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            return
        
        self.state_manager.print_progress_summary()
    
    def get_top_papers_all_time(self, limit: int = 10) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ø —Å—Ç–∞—Ç—å–∏ –∑–∞ –≤—Å–µ –≤—Ä–µ–º—è"""
        if not self.enable_state_tracking or not self.state_manager:
            return []
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏ —Å —Ä–∞–Ω–≥–∞–º–∏
        top_papers = sorted(
            [p for p in self.state_manager.analyzed_papers.values() if p.priority_rank is not None],
            key=lambda x: x.priority_rank or 999
        )[:limit]
        
        return [
            {
                "rank": p.priority_rank,
                "arxiv_id": p.arxiv_id,
                "title": p.title,
                "overall_score": p.overall_score,
                "priority_score": p.priority_score,
                "analysis_date": p.analysis_timestamp,
                "session_id": p.session_id
            }
            for p in top_papers
        ]
    
    def clear_state(self, confirm: bool = False):
        """–û—á–∏—â–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–û–°–¢–û–†–û–ñ–ù–û!)"""
        if not confirm:
            print("‚ö†Ô∏è –î–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–µ—Ä–µ–¥–∞–π—Ç–µ confirm=True")
            return
        
        if not self.enable_state_tracking or not self.state_manager:
            print("‚ö†Ô∏è –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            return
        
        import shutil
        state_path = Path(self.state_manager.state_dir)
        if state_path.exists():
            shutil.rmtree(state_path)
            print(f"üóëÔ∏è –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ—á–∏—â–µ–Ω–æ: {state_path}")
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º state manager —Å —Ç–æ–π –∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            try:
                from .config import get_output_paths, create_output_structure
            except ImportError:
                from config import get_output_paths, create_output_structure
            
            if self.custom_output_dir:
                paths = get_output_paths(self.custom_output_dir)
            else:
                paths = get_output_paths()
            
            create_output_structure(paths["base"])
            self.state_manager = StateManager(str(paths["state"]))
        else:
            print("‚ÑπÔ∏è –°–æ—Å—Ç–æ—è–Ω–∏–µ —É–∂–µ –ø—É—Å—Ç–æ–µ")
    
    def _format_top_papers(self, top_papers: List[RankedPaper]) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–æ–ø —Å—Ç–∞—Ç—å–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        formatted = []
        
        for paper in top_papers:
            formatted.append({
                "rank": paper.priority_rank,
                "score": round(paper.priority_score, 3),
                "title": paper.analysis.paper_info.title,
                "authors": paper.analysis.paper_info.authors[:3],  # –ü–µ—Ä–≤—ã–µ 3 –∞–≤—Ç–æ—Ä–∞
                "arxiv_id": paper.analysis.paper_info.arxiv_id,
                "categories": paper.analysis.paper_info.categories,
                "overall_score": paper.analysis.overall_score,
                "key_insights": paper.analysis.key_insights[:3],  # –ü–µ—Ä–≤—ã–µ 3 –∏–Ω—Å–∞–π—Ç–∞
                "relevance": paper.analysis.relevance_to_task[:200] + "..." if len(paper.analysis.relevance_to_task) > 200 else paper.analysis.relevance_to_task,
                "justification": paper.priority_justification[:300] + "..." if len(paper.priority_justification) > 300 else paper.priority_justification,
                "pdf_url": paper.analysis.paper_info.pdf_url
            })
        
        return formatted
    
    async def save_results(self, results: Dict, filename: Optional[str] = None, custom_dir: Optional[str] = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON —Ñ–∞–π–ª —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∫–∞—Ç–∞–ª–æ–≥–æ–≤"""
        try:
            from .config import (
                get_output_paths, create_output_structure, 
                REPORT_FILENAME_TEMPLATE, TIMESTAMP_FORMAT,
                SAVE_FULL_RESULTS, BACKUP_OLD_REPORTS, MAX_BACKUPS
            )
        except ImportError:
            from config import (
                get_output_paths, create_output_structure,
                REPORT_FILENAME_TEMPLATE, TIMESTAMP_FORMAT, 
                SAVE_FULL_RESULTS, BACKUP_OLD_REPORTS, MAX_BACKUPS
            )
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–∞–ª–æ–≥–æ–≤
        if custom_dir:
            paths = get_output_paths(custom_dir)
        else:
            paths = get_output_paths()
        
        create_output_structure(paths["base"])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        if filename is None:
            timestamp = datetime.now().strftime(TIMESTAMP_FORMAT)
            filename = REPORT_FILENAME_TEMPLATE.format(timestamp=timestamp)
        
        # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        full_path = paths["reports"] / filename
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_data = results.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
        save_data["save_metadata"] = {
            "saved_at": datetime.now().isoformat(),
            "save_path": str(full_path),
            "save_config": {
                "full_results_included": SAVE_FULL_RESULTS,
                "date_structure_used": paths["reports"].parent.name != "reports"
            }
        }
        
        # –£–±–∏—Ä–∞–µ–º full_results –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞
        if not SAVE_FULL_RESULTS and 'full_results' in save_data:
            del save_data['full_results']
            save_data["save_metadata"]["full_results_removed"] = True
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if full_path.exists() and BACKUP_OLD_REPORTS:
            self._create_backup(full_path, MAX_BACKUPS)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            file_size_mb = full_path.stat().st_size / (1024 * 1024)
            print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {full_path}")
            print(f"   üìÅ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.2f} MB")
            print(f"   üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤: {paths['reports'].relative_to(paths['base'])}")
            
            return str(full_path)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {full_path}: {e}")
            return ""
    
    def _create_backup(self, file_path: Path, max_backups: int = 5):
        """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            from pathlib import Path
            
            # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ —Å timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{file_path.stem}_backup_{timestamp}{file_path.suffix}"
            backup_path = file_path.parent / "backups" / backup_name
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π
            backup_path.parent.mkdir(exist_ok=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª
            import shutil
            shutil.copy2(file_path, backup_path)
            print(f"üîÑ –°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_path.name}")
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ –µ—Å–ª–∏ –∏—Ö —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
            self._cleanup_old_backups(backup_path.parent, file_path.stem, max_backups)
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
    
    def _cleanup_old_backups(self, backup_dir: Path, file_stem: str, max_backups: int):
        """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ max_backups"""
        try:
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            pattern = f"{file_stem}_backup_*"
            backups = list(backup_dir.glob(pattern))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
            backups.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∫–æ–ø–∏–∏
            for old_backup in backups[max_backups:]:
                old_backup.unlink()
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {old_backup.name}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π: {e}")
    
    def print_summary(self, results: Dict):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if 'error' in results:
            print(f"‚ùå –û—à–∏–±–∫–∞: {results['error']}")
            return
        
        print("\n" + "="*60)
        print("üìã –°–í–û–î–ö–ê –ê–ù–ê–õ–ò–ó–ê")
        print("="*60)
        
        stats = results['statistics']
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –ó–∞–ø—Ä–æ—Å–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {stats['queries_generated']}")
        print(f"   ‚Ä¢ –°—Ç–∞—Ç–µ–π –Ω–∞–π–¥–µ–Ω–æ: {stats['papers_found']}")
        print(f"   ‚Ä¢ –°—Ç–∞—Ç–µ–π –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {stats['papers_analyzed']}")
        print(f"   ‚Ä¢ –í–∞–ª–∏–¥–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤: {stats['valid_analyses']}")
        print(f"   ‚Ä¢ –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {results['duration_seconds']:.1f} —Å–µ–∫")
        
        print(f"\nüèÜ –¢–û–ü-5 –°–¢–ê–¢–ï–ô:")
        for i, paper in enumerate(results['top_papers'][:5], 1):
            print(f"\n{i}. {paper['title'][:60]}...")
            print(f"   üìà –û—Ü–µ–Ω–∫–∞: {paper['score']:.3f} | arXiv: {paper['arxiv_id']}")
            print(f"   üë• –ê–≤—Ç–æ—Ä—ã: {', '.join(paper['authors'])}")
            print(f"   üí° –ò–Ω—Å–∞–π—Ç: {paper['key_insights'][0] if paper['key_insights'] else '–ù–µ —É–∫–∞–∑–∞–Ω'}")
        
        print("\n" + "="*60)


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    # –ü—Ä–∏–º–µ—Ä 1: –ê–Ω–∞–ª–∏–∑ arXiv —Å—Ç–∞—Ç–µ–π (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å)
    print("=" * 60)
    print("üìñ –ê–ù–ê–õ–ò–ó ARXIV –°–¢–ê–¢–ï–ô")
    print("=" * 60)
    
    analyzer = ArxivAnalyzer()
    
    results = await analyzer.run_full_analysis(
        max_papers_per_query=5,  # –ú–µ–Ω—å—à–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        max_total_papers=20,     # –ú–µ–Ω—å—à–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        use_llm_ranking=True
    )
    
    analyzer.print_summary(results)
    
    if 'error' not in results:
        await analyzer.save_results(results, custom_dir=analyzer.custom_output_dir)
    
    print("\n" + "=" * 60)
    print("üìÅ –ê–ù–ê–õ–ò–ó PDF –§–ê–ô–õ–û–í")
    print("=" * 60)
    
    # –ü—Ä–∏–º–µ—Ä 2: –ê–Ω–∞–ª–∏–∑ PDF —Ñ–∞–π–ª–æ–≤ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–∏
    pdf_analyzer = ArxivAnalyzer(
        pdf_directory="lcgr/downloaded_pdfs/references_dlya_statiy_2025"
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ PDF —Ñ–∞–π–ª–æ–≤
    pdf_results = await pdf_analyzer.run_pdf_analysis(
        max_papers=10,           # –ú–∞–∫—Å–∏–º—É–º PDF –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        use_llm_ranking=True,
        use_cache=True,
        max_workers=4
    )
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    pdf_analyzer.print_summary(pdf_results)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if 'error' not in pdf_results:
        await pdf_analyzer.save_results(pdf_results, filename="pdf_analysis_results.json")


async def analyze_pdf_folder(pdf_directory: str = "lcgr/downloaded_pdfs/references_dlya_statiy_2025"):
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ PDF —Ñ–∞–π–ª–æ–≤ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–∏"""
    print(f"üöÄ –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ PDF —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏: {pdf_directory}")
    
    analyzer = ArxivAnalyzer(pdf_directory=pdf_directory)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
    results = await analyzer.run_pdf_analysis(
        max_papers=100,
        use_llm_ranking=True,
        use_cache=True,
        max_workers=30
    )
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    analyzer.print_summary(results)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if 'error' not in results:
        saved_path = await analyzer.save_results(results, filename="pdf_analysis_results.json")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {saved_path}")
    
    return results


if __name__ == "__main__":
    asyncio.run(main()) 