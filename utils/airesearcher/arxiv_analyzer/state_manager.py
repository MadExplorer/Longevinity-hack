"""
–ú–æ–¥—É–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∞–Ω–∞–ª–∏–∑–∞
"""

import json
import os
import hashlib
from datetime import datetime
from typing import Dict, List, Set, Optional, Any
from dataclasses import dataclass, asdict
from pathlib import Path

try:
    from .models import ArxivQuery, PaperInfo, PaperAnalysis, RankedPaper
except ImportError:
    from models import ArxivQuery, PaperInfo, PaperAnalysis, RankedPaper


@dataclass
class AnalysisSession:
    """–°–µ—Å—Å–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    session_id: str
    timestamp: str
    task_description_hash: str
    queries: List[Dict]
    total_papers_found: int
    completed: bool = False


@dataclass
class PaperState:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ç—å–∏"""
    arxiv_id: str
    title: str
    analysis_timestamp: str
    overall_score: float
    priority_rank: Optional[int] = None
    priority_score: Optional[float] = None
    session_id: str = ""
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å
    analysis_data: Optional[Dict] = None


class StateManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self, state_dir: str = "analysis_state"):
        self.state_dir = Path(state_dir)
        self.state_dir.mkdir(exist_ok=True)
        
        # –§–∞–π–ª—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.sessions_file = self.state_dir / "sessions.json"
        self.papers_file = self.state_dir / "analyzed_papers.json"
        self.queries_cache_file = self.state_dir / "queries_cache.json"
        self.rankings_file = self.state_dir / "rankings_history.json"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.sessions = self._load_sessions()
        self.analyzed_papers = self._load_analyzed_papers()
        self.queries_cache = self._load_queries_cache()
        self.rankings_history = self._load_rankings_history()
    
    def _load_sessions(self) -> List[AnalysisSession]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–µ—Å—Å–∏–π"""
        if not self.sessions_file.exists():
            return []
        
        try:
            with open(self.sessions_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return [AnalysisSession(**session) for session in data]
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–µ—Å—Å–∏–π: {e}")
            return []
    
    def _load_analyzed_papers(self) -> Dict[str, PaperState]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö"""
        if not self.papers_file.exists():
            return {}
        
        try:
            with open(self.papers_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                papers = {}
                for arxiv_id, paper_data in data.items():
                    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - analysis_data –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
                    if 'analysis_data' not in paper_data:
                        paper_data['analysis_data'] = None
                    papers[arxiv_id] = PaperState(**paper_data)
                return papers
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {e}")
            return {}
    
    def _load_queries_cache(self) -> Dict[str, List[Dict]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if not self.queries_cache_file.exists():
            return {}
        
        try:
            with open(self.queries_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞ –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return {}
    
    def _load_rankings_history(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–π"""
        if not self.rankings_file.exists():
            return []
        
        try:
            with open(self.rankings_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–π: {e}")
            return []
    
    def _save_sessions(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–µ—Å—Å–∏–∏"""
        try:
            with open(self.sessions_file, 'w', encoding='utf-8') as f:
                json.dump([asdict(session) for session in self.sessions], f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ—Å—Å–∏–π: {e}")
    
    def _save_analyzed_papers(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏"""
        try:
            with open(self.papers_file, 'w', encoding='utf-8') as f:
                data = {arxiv_id: asdict(paper) for arxiv_id, paper in self.analyzed_papers.items()}
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {e}")
    
    def _save_queries_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            with open(self.queries_cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.queries_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞ –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
    
    def _save_rankings_history(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–π"""
        try:
            with open(self.rankings_file, 'w', encoding='utf-8') as f:
                json.dump(self.rankings_history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–π: {e}")
    
    def get_task_hash(self, task_description: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à –æ–ø–∏—Å–∞–Ω–∏—è –∑–∞–¥–∞—á–∏"""
        return hashlib.md5(task_description.encode('utf-8')).hexdigest()[:8]
    
    def get_cached_queries(self, task_hash: str) -> Optional[List[ArxivQuery]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∑–∞–¥–∞—á–∏"""
        if task_hash in self.queries_cache:
            cached_data = self.queries_cache[task_hash]
            try:
                from .models import SearchStrategy
                queries = []
                for query_data in cached_data:
                    strategy = SearchStrategy(query_data["strategy"])
                    query = ArxivQuery(
                        strategy=strategy,
                        query=query_data["query"],
                        max_results=query_data.get("max_results", 10)
                    )
                    queries.append(query)
                return queries
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
                return None
        return None
    
    def cache_queries(self, task_hash: str, queries: List[ArxivQuery]):
        """–ö—ç—à–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∑–∞–¥–∞—á–∏"""
        self.queries_cache[task_hash] = [
            {
                "strategy": q.strategy.value,
                "query": q.query,
                "max_results": q.max_results
            }
            for q in queries
        ]
        self._save_queries_cache()
    
    def start_session(self, task_description: str, queries: List[ArxivQuery]) -> str:
        """–ù–∞—á–∏–Ω–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –∞–Ω–∞–ª–∏–∑–∞"""
        session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        task_hash = self.get_task_hash(task_description)
        
        session = AnalysisSession(
            session_id=session_id,
            timestamp=datetime.now().isoformat(),
            task_description_hash=task_hash,
            queries=[
                {"strategy": q.strategy.value, "query": q.query}
                for q in queries
            ],
            total_papers_found=0,
            completed=False
        )
        
        self.sessions.append(session)
        self._save_sessions()
        return session_id
    
    def complete_session(self, session_id: str, total_papers: int):
        """–ó–∞–≤–µ—Ä—à–∞–µ—Ç —Å–µ—Å—Å–∏—é"""
        for session in self.sessions:
            if session.session_id == session_id:
                session.completed = True
                session.total_papers_found = total_papers
                break
        self._save_sessions()
    
    def is_paper_analyzed(self, arxiv_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ —Å—Ç–∞—Ç—å—è —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞"""
        return arxiv_id in self.analyzed_papers
    
    def get_analyzed_paper(self, arxiv_id: str) -> Optional[PaperState]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ"""
        return self.analyzed_papers.get(arxiv_id)
    
    def get_full_analysis(self, arxiv_id: str) -> Optional[PaperAnalysis]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏, –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –æ–±—ä–µ–∫—Ç PaperAnalysis"""
        paper_state = self.analyzed_papers.get(arxiv_id)
        if not paper_state or not paper_state.analysis_data:
            return None
        
        try:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PaperAnalysis –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            analysis_data = paper_state.analysis_data
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏
            try:
                from .models import PaperAnalysis
            except ImportError:
                from models import PaperAnalysis
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç PaperAnalysis –∏–∑ —Å–ª–æ–≤–∞—Ä—è
            if hasattr(PaperAnalysis, 'model_validate'):
                # –î–ª—è Pydantic v2
                return PaperAnalysis.model_validate(analysis_data)
            else:
                # –î–ª—è Pydantic v1 –∏–ª–∏ –¥—Ä—É–≥–∏—Ö —Å–ª—É—á–∞–µ–≤
                return PaperAnalysis(**analysis_data)
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {arxiv_id}: {e}")
            return None
    
    def save_paper_analysis(self, analysis: PaperAnalysis, session_id: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏"""
        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –∞–Ω–∞–ª–∏–∑ –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è JSON —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        analysis_dict = None
        try:
            analysis_dict = analysis.model_dump() if hasattr(analysis, 'model_dump') else asdict(analysis)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑: {e}")
        
        paper_state = PaperState(
            arxiv_id=analysis.paper_info.arxiv_id,
            title=analysis.paper_info.title,
            analysis_timestamp=datetime.now().isoformat(),
            overall_score=analysis.overall_score,
            session_id=session_id,
            analysis_data=analysis_dict
        )
        
        self.analyzed_papers[analysis.paper_info.arxiv_id] = paper_state
        self._save_analyzed_papers()
    
    def update_paper_ranking(self, ranked_paper: RankedPaper):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        arxiv_id = ranked_paper.analysis.paper_info.arxiv_id
        if arxiv_id in self.analyzed_papers:
            self.analyzed_papers[arxiv_id].priority_rank = ranked_paper.priority_rank
            self.analyzed_papers[arxiv_id].priority_score = ranked_paper.priority_score
            self._save_analyzed_papers()
    
    def save_ranking_session(self, ranked_papers: List[RankedPaper], session_id: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        ranking_data = {
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "total_papers": len(ranked_papers),
            "top_5": [
                {
                    "rank": paper.priority_rank,
                    "arxiv_id": paper.analysis.paper_info.arxiv_id,
                    "title": paper.analysis.paper_info.title,
                    "score": paper.priority_score
                }
                for paper in ranked_papers[:5]
            ]
        }
        
        self.rankings_history.append(ranking_data)
        self._save_rankings_history()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö
        for ranked_paper in ranked_papers:
            self.update_paper_ranking(ranked_paper)
    
    def filter_new_papers(self, papers: List[PaperInfo]) -> List[PaperInfo]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–æ–≤—ã–µ (–Ω–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ) —Å—Ç–∞—Ç—å–∏"""
        new_papers = []
        for paper in papers:
            if not self.is_paper_analyzed(paper.arxiv_id):
                new_papers.append(paper)
        return new_papers
    
    def get_progress_summary(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –ø—Ä–æ–≥—Ä–µ—Å—Å—É"""
        total_sessions = len(self.sessions)
        completed_sessions = len([s for s in self.sessions if s.completed])
        total_papers = len(self.analyzed_papers)
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–µ—Å—Å–∏—è
        last_session = self.sessions[-1] if self.sessions else None
        
        # –¢–æ–ø —Å—Ç–∞—Ç—å–∏ –ø–æ –≤—Å–µ–º —Å–µ—Å—Å–∏—è–º
        top_papers = sorted(
            [p for p in self.analyzed_papers.values() if p.priority_rank is not None],
            key=lambda x: x.priority_rank or 999
        )[:5]
        
        return {
            "total_sessions": total_sessions,
            "completed_sessions": completed_sessions,
            "total_analyzed_papers": total_papers,
            "last_session": {
                "id": last_session.session_id if last_session else None,
                "timestamp": last_session.timestamp if last_session else None,
                "completed": last_session.completed if last_session else None
            } if last_session else None,
            "top_papers": [
                {
                    "rank": p.priority_rank,
                    "arxiv_id": p.arxiv_id,
                    "title": p.title,
                    "overall_score": p.overall_score
                }
                for p in top_papers
            ]
        }
    
    def print_progress_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        summary = self.get_progress_summary()
        
        print("\n" + "="*50)
        print("üìà –°–í–û–î–ö–ê –ü–†–û–ì–†–ï–°–°–ê")
        print("="*50)
        
        print(f"üìä –°–µ—Å—Å–∏–∏: {summary['completed_sessions']}/{summary['total_sessions']} –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        print(f"üìö –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {summary['total_analyzed_papers']}")
        
        if summary['last_session']:
            status = "‚úÖ –∑–∞–≤–µ—Ä—à–µ–Ω–∞" if summary['last_session']['completed'] else "‚è≥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ"
            print(f"üïê –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–µ—Å—Å–∏—è: {summary['last_session']['id']} ({status})")
        
        if summary['top_papers']:
            print(f"\nüèÜ –¢–û–ü-{len(summary['top_papers'])} –°–¢–ê–¢–ï–ô (–ø–æ –≤—Å–µ–º —Å–µ—Å—Å–∏—è–º):")
            for paper in summary['top_papers']:
                print(f"   {paper['rank']}. {paper['title'][:50]}...")
                print(f"      üìà –û—Ü–µ–Ω–∫–∞: {paper['overall_score']:.3f} | arXiv: {paper['arxiv_id']}")
        
        print("="*50)


def main():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    manager = StateManager()
    manager.print_progress_summary()


if __name__ == "__main__":
    main() 