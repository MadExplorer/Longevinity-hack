# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
–ü—Ä–æ—Å—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PDF, JSON —Ñ–∞–π–ª–∞–º–∏ –∏ –¥—Ä—É–≥–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
"""

import json
from pathlib import Path
from tqdm import tqdm
import concurrent.futures

from .pdf_processing import SimplePDFReader, CacheManager

def process_single_pdf(pdf_file, cache, pdf_reader):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω PDF —Ñ–∞–π–ª (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏)"""
    paper_id = f"PDF_{pdf_file.stem}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if cache:
        cached_text = cache.get_pdf_text(str(pdf_file))
        if cached_text:
            print(f"  üìÅ {paper_id}: –∏–∑ –∫—ç—à–∞")
            return paper_id, cached_text, 2024
        else:
            print(f"  üîÑ {paper_id}: —á–∏—Ç–∞–µ–º PDF...")
            full_text = pdf_reader.read_pdf(str(pdf_file))
            if full_text:
                cache.save_pdf_text(str(pdf_file), full_text)
            return paper_id, full_text, 2024
    else:
        full_text = pdf_reader.read_pdf(str(pdf_file))
        return paper_id, full_text, 2024

def load_harvester_data(data_path, use_cache=True, max_workers=4):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ—Ç harvester (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç lcgr_ready_*.json)"""
    print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç harvester: {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    documents = {}
    cache = CacheManager() if use_cache else None
    pdf_reader = SimplePDFReader()
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ PDF –∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
    pdf_papers = [(paper_id, doc_data) for paper_id, doc_data in data.items() if doc_data.get('has_pdf')]
    text_papers = [(paper_id, doc_data) for paper_id, doc_data in data.items() if not doc_data.get('has_pdf')]
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(pdf_papers)} PDF + {len(text_papers)} —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç")
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF —Ñ–∞–π–ª—ã
    if pdf_papers:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_paper = {
                executor.submit(process_single_pdf, Path(pdf_path), cache, pdf_reader): (paper_id, doc_data) 
                for paper_id, doc_data in pdf_papers
            }
            
            for future in tqdm(concurrent.futures.as_completed(future_to_paper), 
                             total=len(pdf_papers), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–æ–≤",
                             position=0, leave=True):
                paper_id, doc_data = future_to_paper[future]
                try:
                    _, full_text, _ = future.result()
                    if full_text:
                        documents[paper_id] = {
                            "full_text": full_text,
                            "year": doc_data.get('year', 2024)
                        }
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF {paper_id}: {e}")
                    # Fallback –Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
                    documents[paper_id] = {
                        "full_text": doc_data.get('abstract', ''),
                        "year": doc_data.get('year', 2024)
                    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ —Å —Ç–µ–∫—Å—Ç–æ–º
    for paper_id, doc_data in text_papers:
        documents[paper_id] = {
            "full_text": doc_data.get('full_text', ''),
            "year": doc_data.get('year', 2024)
        }
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ harvester")
    return documents

def load_pdf_directory(data_path, use_cache=True, max_workers=4):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç PDF —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏"""
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ —Å PDF: {data_path}")
    
    documents = {}
    pdf_files = list(data_path.glob("*.pdf"))
    
    cache = CacheManager() if use_cache else None
    pdf_reader = SimplePDFReader()
    
    print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤ (–ø–æ—Ç–æ–∫–æ–≤: {max_workers})")
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–æ–≤
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_pdf = {
            executor.submit(process_single_pdf, pdf_file, cache, pdf_reader): pdf_file 
            for pdf_file in pdf_files
        }
        
        for future in tqdm(concurrent.futures.as_completed(future_to_pdf), 
                         total=len(pdf_files), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–æ–≤",
                         position=0, leave=True):
            pdf_file = future_to_pdf[future]
            try:
                paper_id, full_text, year = future.result()
                if full_text:
                    documents[paper_id] = {
                        "full_text": full_text,
                        "year": year
                    }
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {pdf_file}: {e}")
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} PDF —Ñ–∞–π–ª–æ–≤")
    return documents

def load_old_json_file(data_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ä—ã–π JSON —Ñ–∞–π–ª (pubmed_corpus.json)"""
    print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—ã–π JSON —Ñ–∞–π–ª: {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    converted = {}
    for paper_id, doc_data in data.items():
        converted[paper_id] = {
            "full_text": doc_data.get('abstract', doc_data.get('full_text', '')),
            "year": doc_data.get('year', 2024)
        }
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(converted)} —Å—Ç–∞—Ç–µ–π –∏–∑ JSON")
    return converted

def create_test_data():
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"""
    print("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
    
    return {
        "TEST:1": {
            "full_text": "Here we test the hypothesis that SIRT1 activation extends lifespan. Using CRISPR-Cas9 as our primary method, we observed a 20% increase in lifespan in C. elegans. We conclude that SIRT1 is a key longevity gene.",
            "year": 2023
        },
        "TEST:2": {
            "full_text": "The role of mTOR in aging is well-established. We hypothesized that inhibiting mTOR with Rapamycin would reduce cellular senescence. Our results, obtained via flow cytometry, showed a significant decrease in senescent cells. This confirms the link between mTOR and senescence.",
            "year": 2022
        },
        "TEST:3": {
            "full_text": "This paper explores if SIRT1 is involved in cellular metabolism. We propose that it is a master regulator. However, our experiments using mass spectrometry did not yield conclusive results connecting it directly to glucose uptake.",
            "year": 2021
        },
        "TEST:4": {
            "full_text": "We investigated the protein landscape of senescent cells using a novel single-cell proteomics method. Our analysis revealed that mTOR signaling is consistently upregulated. We hypothesize this is a core driver of the senescent phenotype.",
            "year": 2024
        }
    }

def load_documents(data_source="downloaded_pdfs", use_cache=True, max_workers=4):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    data_path = Path(data_source)
    
    # 1. –ï—Å–ª–∏ —ç—Ç–æ JSON —Ñ–∞–π–ª –æ—Ç harvester (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç)
    if data_path.is_file() and data_path.suffix == '.json' and 'lcgr_ready' in data_path.name:
        return load_harvester_data(data_path, use_cache, max_workers)
    
    # 2. –ï—Å–ª–∏ —ç—Ç–æ –ø–∞–ø–∫–∞ —Å PDF —Ñ–∞–π–ª–∞–º–∏
    elif data_path.is_dir():
        return load_pdf_directory(data_path, use_cache, max_workers)
    
    # 3. –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç–∞—Ä—ã–π JSON —Ñ–∞–π–ª (pubmed_corpus.json)
    elif data_path.is_file() and data_path.suffix == '.json':
        return load_old_json_file(data_path)
    
    # 4. –ê–≤—Ç–æ–ø–æ–∏—Å–∫ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
    else:
        print(f"üîç –ê–≤—Ç–æ–ø–æ–∏—Å–∫ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã harvester (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        harvester_files = list(Path(".").glob("lcgr_ready_*.json"))
        if harvester_files:
            latest_file = max(harvester_files, key=lambda p: p.stat().st_mtime)
            print(f"üìÑ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª harvester: {latest_file}")
            return load_documents(str(latest_file), use_cache, max_workers)
        
        # –ò—â–µ–º –ø–∞–ø–∫—É —Å PDF
        possible_paths = [
            Path("downloaded_pdfs"),
            Path(".") / "downloaded_pdfs", 
            Path("..") / "downloaded_pdfs",
        ]
        
        for path in possible_paths:
            if path.exists() and path.is_dir() and list(path.glob("*.pdf")):
                print(f"üìÅ –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ —Å PDF: {path}")
                return load_documents(str(path), use_cache, max_workers)
        
        # –ò—â–µ–º —Å—Ç–∞—Ä—ã–µ JSON —Ñ–∞–π–ª—ã
        old_json_files = ["pubmed_corpus.json", "arxiv_corpus.json", "combined_corpus.json"]
        for json_file in old_json_files:
            if Path(json_file).exists():
                print(f"üìÑ –ù–∞–π–¥–µ–Ω JSON —Ñ–∞–π–ª: {json_file}")
                return load_documents(json_file, use_cache, max_workers)
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        return create_test_data() 