# -*- coding: utf-8 -*-
"""
–ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
–ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥—Ä–∞—Ñ–æ–º –∑–Ω–∞–Ω–∏–π
"""

import networkx as nx
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm
import concurrent.futures

from core.models import ExtractedKnowledge
from processing.pdf_processing import SimplePDFReader, CacheManager
from config import llm_extractor_client
from .entity_normalizer import EntityNormalizer

class ScientificKnowledgeGraph:
    """–ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
        self.pdf_reader = SimplePDFReader()
        self.cache = CacheManager()
        self.entity_normalizer = EntityNormalizer()

    def save_graph(self, filepath: str = "knowledge_graph.graphml"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ –≤ —Ñ–∞–π–ª GraphML"""
        try:
            filepath = Path(filepath)
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            nx.write_graphml(self.graph, filepath)
            print(f"üíæ –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filepath}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞: {e}")
            return False

    def load_graph(self, filepath: str = "knowledge_graph.graphml") -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≥—Ä–∞—Ñ –∏–∑ —Ñ–∞–π–ª–∞ GraphML"""
        try:
            filepath = Path(filepath)
            if not filepath.exists():
                print(f"üìÅ –§–∞–π–ª –≥—Ä–∞—Ñ–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
                return False
            
            self.graph = nx.read_graphml(filepath)
            print(f"‚úÖ –ì—Ä–∞—Ñ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞: {filepath}")
            print(f"   üìä –£–∑–ª–æ–≤: {self.graph.number_of_nodes()}, –†—ë–±–µ—Ä: {self.graph.number_of_edges()}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥—Ä–∞—Ñ–∞: {e}")
            return False

    def get_graph_stats(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä–∞—Ñ–∞"""
        if not self.graph:
            return "–ì—Ä–∞—Ñ –ø—É—Å—Ç"
        
        stats = {
            'nodes': self.graph.number_of_nodes(),
            'edges': self.graph.number_of_edges(),
            'papers': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Paper']),
            'hypotheses': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Hypothesis']),
            'methods': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Method']),
            'results': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Result']),
            'conclusions': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Conclusion']),
            'entities': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Entity'])
        }
        return stats

    def _extract_scientific_concepts(self, paper_id: str, text: str) -> ExtractedKnowledge:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞—É—á–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏"""
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
        text_for_prompt = text[:200000] + ("..." if len(text) > 80000 else "")
        
        prompt_text = f"""
        You are an expert in scientific research methodology and bioinformatics.
        
        TASK: Analyze the following FULL scientific paper text and extract its core components.
        
        IMPORTANT DISTINCTIONS:
        - Hypothesis: A testable prediction or proposed explanation (often starts with "we hypothesize", "we propose", "we test the hypothesis")
        - Method: The experimental technique or approach used (e.g., "using CRISPR", "via flow cytometry", "mass spectrometry")  
        - Result: The actual findings or observations from experiments (e.g., "we observed", "showed", "revealed")
        - Conclusion: Final interpretations or implications drawn from results (e.g., "we conclude", "this confirms")
        
        For each component, identify all mentioned biological entities (Genes like SIRT1, Proteins like mTOR, Diseases, Compounds like Rapamycin, Processes like senescence).
        
        BE PRECISE: A hypothesis without corresponding results in the same paper should remain unconnected.

        CRITICAL: Your response MUST be a structured JSON that follows the ExtractedKnowledge schema.
        CRITICAL: All text fields (statements, entity names) MUST be in English only.

        FULL PAPER TEXT: "{text_for_prompt}"
        
        Paper ID: {paper_id}
        """
        
        try:
            knowledge = llm_extractor_client.chat.completions.create(
                messages=[{"role": "user", "content": prompt_text}],
                response_model=ExtractedKnowledge
            )
            knowledge.paper_id = paper_id
            return knowledge
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –¥–ª—è {paper_id}: {e}")
            return ExtractedKnowledge(paper_id=paper_id, concepts=[])

    def _process_single_document(self, paper_id, doc_data):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"""
        year = doc_data.get('year', 2024)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å PDF —Ñ–∞–π–ª - –∏–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ü–µ–ø—Ç—ã –Ω–∞–ø—Ä—è–º—É—é –∏–∑ PDF
        if doc_data.get('has_pdf') and doc_data.get('pdf_path'):
            print(f"  üìÑ {paper_id}: –ø—Ä—è–º–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ PDF")
            extracted_knowledge = self.pdf_reader.extract_concepts_from_pdf(
                doc_data['pdf_path'], paper_id
            )
            text = f"Processed from PDF: {doc_data['pdf_path']}"
        else:
            # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            text = doc_data['full_text']
            extracted_knowledge = self._extract_scientific_concepts(paper_id, text)
        
        if extracted_knowledge:
            print(f"  üìÑ {paper_id}: –Ω–∞–π–¥–µ–Ω–æ {len(extracted_knowledge.concepts)} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤")
        else:
            print(f"  ‚ùå {paper_id}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ü–µ–ø—Ç—ã")
            extracted_knowledge = ExtractedKnowledge(paper_id=paper_id, concepts=[])
        
        return paper_id, text, year, extracted_knowledge

    def build_graph(self, documents: dict, max_workers=4, force_rebuild_normalization=False):
        """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ—Ç–æ–∫–æ–≤: {max_workers})")
        
        # –§–∞–∑–∞ 1: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
        all_results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_paper = {
                executor.submit(self._process_single_document, paper_id, doc_data): paper_id 
                for paper_id, doc_data in documents.items()
            }
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –±–µ–∑ desc –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏
            for future in tqdm(concurrent.futures.as_completed(future_to_paper), 
                             total=len(documents), desc="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤", 
                             position=0, leave=True):
                paper_id = future_to_paper[future]
                try:
                    result = future.result()
                    all_results.append(result)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {paper_id}: {e}")
        
        # –§–∞–∑–∞ 2: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        print("\nüîÑ –§–∞–∑–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π...")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        all_extracted_knowledge = [result[3] for result in all_results]  # result[3] = extracted_knowledge
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        unique_entities = self.entity_normalizer.collect_all_entities(all_extracted_knowledge)
        
        if unique_entities:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–∞–ø–∏–Ω–≥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            normalization_file = "entity_normalization_map.json"
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –º–∞–ø–∏–Ω–≥ –ø—Ä–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–º –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–∏
            if force_rebuild_normalization and Path(normalization_file).exists():
                print(f"üóëÔ∏è –£–¥–∞–ª—è—é —Å—Ç–∞—Ä—ã–π –º–∞–ø–∏–Ω–≥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {normalization_file}")
                Path(normalization_file).unlink()
            
            if Path(normalization_file).exists() and not force_rebuild_normalization:
                print(f"üìÅ –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–∞–ø–∏–Ω–≥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {normalization_file}")
                if self.entity_normalizer.load_mapping(normalization_file):
                    print("   ‚úÖ –ú–∞–ø–∏–Ω–≥ –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                else:
                    print("   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –º–∞–ø–∏–Ω–≥")
                    self.entity_normalizer.normalize_entities(unique_entities)
                    self.entity_normalizer.save_mapping(normalization_file)
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –º–∞–ø–∏–Ω–≥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                print("üÜï –°–æ–∑–¥–∞—é –Ω–æ–≤—ã–π –º–∞–ø–∏–Ω–≥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏...")
                self.entity_normalizer.normalize_entities(unique_entities)
                self.entity_normalizer.save_mapping(normalization_file)
                
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            self.entity_normalizer.print_statistics()
        else:
            print("   ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
        
        # –§–∞–∑–∞ 3: –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏
        print("\nüèóÔ∏è –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤...")
        
        for paper_id, text, year, extracted_knowledge in tqdm(all_results, desc="–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞"):
            # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–µ–ª –¥–ª—è —Å—Ç–∞—Ç—å–∏
            self.graph.add_node(paper_id, type='Paper', content=text[:500], year=year)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ü–µ–ø—Ç—ã –∏ —Å–≤—è–∑–∏
            for concept in extracted_knowledge.concepts:
                concept_id = f"{paper_id}_{concept.concept_type}_{hash(concept.statement)}"
                self.graph.add_node(concept_id, 
                                  type=concept.concept_type, 
                                  content=concept.statement, 
                                  statement=concept.statement,
                                  paper_id=paper_id)
                self.graph.add_edge(paper_id, concept_id, type='CONTAINS')
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
                for entity in concept.mentioned_entities:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –∏–º—è –≤–º–µ—Å—Ç–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ
                    canonical_name = self.entity_normalizer.get_canonical_name(entity.name)
                    entity_id = f"{entity.type}_{canonical_name.upper()}"
                    
                    if not self.graph.has_node(entity_id):
                        self.graph.add_node(entity_id, 
                                          type='Entity', 
                                          entity_type=entity.type, 
                                          name=canonical_name.upper(),
                                          canonical_name=canonical_name)
                    self.graph.add_edge(concept_id, entity_id, type='MENTIONS', context=concept.statement)

    def visualize_graph(self):
        """–ü—Ä–æ—Å—Ç–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞"""
        plt.figure(figsize=(12, 8))
        pos = nx.spring_layout(self.graph, k=1, iterations=50)
        
        # –¶–≤–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —É–∑–ª–æ–≤
        colors = {
            'Paper': 'lightblue', 
            'Hypothesis': 'lightgreen', 
            'Method': 'lightyellow', 
            'Result': 'lightcoral', 
            'Conclusion': 'lightpink', 
            'Entity': 'lightgray'
        }
        
        for node_type in colors:
            nodes = [n for n, data in self.graph.nodes(data=True) if data.get('type') == node_type]
            nx.draw_networkx_nodes(self.graph, pos, nodelist=nodes, 
                                 node_color=colors[node_type], 
                                 node_size=300, alpha=0.8)
        
        nx.draw_networkx_edges(self.graph, pos, alpha=0.5, arrows=True)
        plt.title("Scientific Knowledge Graph")
        plt.axis('off')
        plt.show() 