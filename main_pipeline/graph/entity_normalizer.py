# -*- coding: utf-8 -*-
"""
–ê–≥–µ–Ω—Ç-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
"""

import json
from typing import Dict, List, Set
from google import genai

class EntityNormalizer:
    """
    –ê–≥–µ–Ω—Ç-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
    –ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.normalization_map = {}
        self.reverse_map = {}  # –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –∏–º–µ–Ω–∏
        self._google_client = None  # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    
    @property
    def google_client(self):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google –∫–ª–∏–µ–Ω—Ç–∞"""
        if self._google_client is None:
            self._google_client = genai.Client()
        return self._google_client
    
    def collect_all_entities(self, documents_knowledge: List) -> List[str]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            documents_knowledge: –°–ø–∏—Å–æ–∫ ExtractedKnowledge –æ–±—ä–µ–∫—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–º–µ–Ω —Å—É—â–Ω–æ—Å—Ç–µ–π
        """
        print("üìã –°–æ–±–∏—Ä–∞—é –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        all_entity_names = set()
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        for doc_knowledge in documents_knowledge:
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –∫–æ–Ω—Ü–µ–ø—Ç–∞–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            for concept in doc_knowledge.concepts:
                # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —Å—É—â–Ω–æ—Å—Ç—è–º –≤ –∫–æ–Ω—Ü–µ–ø—Ç–µ
                for entity in concept.mentioned_entities:
                    all_entity_names.add(entity.name)
        
        unique_entities = list(all_entity_names)
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_entities)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π")
        
        return unique_entities
    
    def normalize_entities(self, entity_names: List[str]) -> Dict[str, List[str]]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é LLM
        
        Args:
            entity_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –∏–º–µ–Ω –∏ –∏—Ö —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        """
        print("ü§ñ –ó–∞–ø—É—Å–∫–∞—é –∞–≥–µ–Ω—Ç–∞-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")
        
        # Prompt for entity normalizer agent
        prompt = """# ROLE
You are a world-class bioinformatics expert specializing in biological entity normalization and ontologies. Your task is to analyze a list of terms extracted from a corpus of longevity research papers and group them by canonical (commonly accepted) names.

# TASK
Analyze the following JSON array of entity names. Some of them are synonyms, abbreviations, or simply variations of the same thing. Group them together. If you are not sure, leave the entity in its own group.

# INPUT DATA (ENTITY LIST)
{entity_list}

# OUTPUT INSTRUCTIONS
Your response MUST be a JSON array of objects with canonical names and their aliases.

# EXAMPLE OUTPUT
[
  {{"canonical_name": "GLP-1R", "aliases": ["GLP-1R", "GLP1R", "Glucagon-like peptide-1 receptor"]}},
  {{"canonical_name": "Resveratrol", "aliases": ["RESVERATROL", "RSV", "resv"]}},
  {{"canonical_name": "NPY", "aliases": ["NPY", "Neuropeptide Y"]}},
  {{"canonical_name": "cfChPs", "aliases": ["cfChPs"]}}
]

# IMPORTANT RULES:
1. Use the most common/official name as canonical
2. Group only if you are 90%+ confident
3. Include the canonical name in the aliases list
4. Do not invent new groups - work only with the given entities"""

        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è LLM
            entity_list_json = json.dumps(entity_names, ensure_ascii=False, indent=2)
            
            # –í—ã–∑—ã–≤–∞–µ–º LLM —Å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤—ã–≤–æ–¥–æ–º (–Ω–∞—Ç–∏–≤–Ω—ã–π Google API)
            response = self.google_client.models.generate_content(
                # model="gemini-2.5-flash"
                model="gemini-2.5-flash",
                contents=prompt.format(entity_list=entity_list_json),
                config={
                    "response_mime_type": "application/json",
                    "response_schema": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "canonical_name": {"type": "string"},
                                "aliases": {
                                    "type": "array",
                                    "items": {"type": "string"}
                                }
                            },
                            "required": ["canonical_name", "aliases"]
                        }
                    }
                }
            )
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
            entities_array = json.loads(response.text)
            normalization_map = {}
            for item in entities_array:
                canonical = item["canonical_name"]
                aliases = item["aliases"]
                normalization_map[canonical] = aliases
            
            print(f"   ‚úÖ –ê–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–ª {len(normalization_map)} –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø")
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            reverse_map = {}
            for canonical_name, aliases in normalization_map.items():
                for alias in aliases:
                    reverse_map[alias] = canonical_name
            
            self.normalization_map = normalization_map
            self.reverse_map = reverse_map
            
            return normalization_map
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            print("   ‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é –∏—Å—Ö–æ–¥–Ω—ã–µ –∏–º–µ–Ω–∞ –±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
            
            # Fallback: –∫–∞–∂–¥–∞—è —Å—É—â–Ω–æ—Å—Ç—å —Å–∞–º–∞ —Å–µ–±–µ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
            fallback_map = {name: [name] for name in entity_names}
            self.normalization_map = fallback_map
            self.reverse_map = {name: name for name in entity_names}
            
            return fallback_map
    
    def get_canonical_name(self, entity_name: str) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –∏–º—è –¥–ª—è –¥–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏
        
        Args:
            entity_name: –ò–º—è —Å—É—â–Ω–æ—Å—Ç–∏
            
        Returns:
            –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –∏–º—è —Å—É—â–Ω–æ—Å—Ç–∏
        """
        return self.reverse_map.get(entity_name, entity_name)
    
    def save_mapping(self, file_path: str) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–∞–ø–∏–Ω–≥ –≤ JSON —Ñ–∞–π–ª
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            True –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.normalization_map, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–ø–∏–Ω–≥–∞: {e}")
            return False
    
    def load_mapping(self, file_path: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞–ø–∏–Ω–≥ –∏–∑ JSON —Ñ–∞–π–ª–∞
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                self.normalization_map = json.load(f)
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
            self.reverse_map = {}
            for canonical_name, aliases in self.normalization_map.items():
                for alias in aliases:
                    self.reverse_map[alias] = canonical_name
            
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–∞–ø–∏–Ω–≥–∞: {e}")
            return False
    
    def print_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if not self.normalization_map:
            print("   ‚ö†Ô∏è –ú–∞–ø–∏–Ω–≥ –Ω–µ —Å–æ–∑–¥–∞–Ω")
            return
        
        total_entities = sum(len(aliases) for aliases in self.normalization_map.values())
        canonical_count = len(self.normalization_map)
        
        print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
        print(f"      ‚Ä¢ –í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {total_entities}")
        print(f"      ‚Ä¢ –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø: {canonical_count}")
        print(f"      ‚Ä¢ –°—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è: {total_entities/canonical_count:.1f}x")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≥—Ä—É–ø–ø
        print(f"   üîç –ü—Ä–∏–º–µ—Ä—ã –≥—Ä—É–ø–ø:")
        count = 0
        for canonical, aliases in self.normalization_map.items():
            if len(aliases) > 1 and count < 3:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –≥—Ä—É–ø–ø—ã —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
                print(f"      ‚Ä¢ {canonical}: {aliases}")
                count += 1 


def normalize_entities_simple(documents_knowledge: List, save_file: str = "entity_normalization_map.json") -> EntityNormalizer:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π
    –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö
    
    Args:
        documents_knowledge: –°–ø–∏—Å–æ–∫ ExtractedKnowledge –æ–±—ä–µ–∫—Ç–æ–≤
        save_file: –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–ø–∏–Ω–≥–∞
        
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π EntityNormalizer
    """
    normalizer = EntityNormalizer()
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
    unique_entities = normalizer.collect_all_entities(documents_knowledge)
    
    if unique_entities:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        normalizer.normalize_entities(unique_entities)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        if save_file:
            normalizer.save_mapping(save_file)
            print(f"üíæ –ú–∞–ø–∏–Ω–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_file}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        normalizer.print_statistics()
    
    return normalizer 