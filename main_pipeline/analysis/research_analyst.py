# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –∞–Ω–∞–ª–∏–∑–∞ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
"""

import json
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import concurrent.futures
import numpy as np

from core.models import Critique, PrioritizedDirection, SynthesizedBridgeIdea, ThematicProgram, HierarchicalReport, DirectionSubgroup, DirectionType
from config import llm_critic_client

class ResearchAnalyst:
    """–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, knowledge_graph):
        self.graph = knowledge_graph.graph

    def _generate_directions_from_white_spots(self, max_workers=4) -> list:
        """–ü–æ–∏—Å–∫ '–±–µ–ª—ã—Ö –ø—è—Ç–µ–Ω' —Å –ø–æ–º–æ—â—å—é –ê–≥–µ–Ω—Ç–∞-–°–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π"""
        print("  üî¨ –ó–∞–ø—É—Å–∫–∞—é –ê–≥–µ–Ω—Ç–∞-–°–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–µ–ª—ã—Ö –ø—è—Ç–µ–Ω...")
        directions = []
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≥–∏–ø–æ—Ç–µ–∑—ã
        all_hypotheses = [(node_id, data) for node_id, data in self.graph.nodes(data=True) 
                         if data.get('type') == 'Hypothesis']
        print(f"     üîç –ù–∞–π–¥–µ–Ω–æ –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(all_hypotheses)}")
        
        # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        whitespot_tasks = []
        for node_id, data in all_hypotheses:
            successors = list(self.graph.successors(node_id))
            has_results = any(self.graph.nodes[n].get('type') == 'Result' 
                            for n in successors)
            
            if not has_results:
                paper_id = data.get('paper_id')
                paper_node = self.graph.nodes.get(paper_id, {})
                paper_context = paper_node.get('content', '')
                hypothesis_text = data.get('statement', data.get('content', 'N/A'))
                
                whitespot_tasks.append({
                    'hypothesis_text': hypothesis_text,
                    'paper_id': paper_id,
                    'paper_context': paper_context
                })
        
        print(f"     üìã –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(whitespot_tasks)} –∑–∞–¥–∞—á –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ ({max_workers} –ø–æ—Ç–æ–∫–æ–≤)")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ–∑–∞ –±–µ–ª—ã—Ö –ø—è—Ç–µ–Ω
        def process_whitespot(task):
            idea = self._synthesize_whitespot_idea(
                task['hypothesis_text'], 
                task['paper_id'], 
                task['paper_context']
            )
            if idea:
                return {
                    "type": "White Spot",
                    "title": idea.title,
                    "description": f"{idea.scientific_premise} {idea.proposed_direction}",
                    "supporting_papers": [task['paper_id']]
                }
            return None
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {
                executor.submit(process_whitespot, task): task 
                for task in whitespot_tasks
            }
            
            for future in tqdm(concurrent.futures.as_completed(future_to_task), 
                             total=len(whitespot_tasks), desc="–ê–Ω–∞–ª–∏–∑ –±–µ–ª—ã—Ö –ø—è—Ç–µ–Ω",
                             position=0, leave=True):
                try:
                    result = future.result()
                    if result:
                        directions.append(result)
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–µ–ª–æ–≥–æ –ø—è—Ç–Ω–∞: {e}")
        
        print(f"     ‚úÖ –°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–æ {len(directions)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –±–µ–ª—ã—Ö –ø—è—Ç–µ–Ω")
        return directions

    def _generate_directions_from_bridges(self, max_workers=4) -> list:
        """–ü–æ–∏—Å–∫ '–º–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã—Ö –º–æ—Å—Ç–æ–≤' —Å –ø–æ–º–æ—â—å—é –ê–≥–µ–Ω—Ç–∞-–°–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞."""
        print("  üß† –ó–∞–ø—É—Å–∫–∞—é –ê–≥–µ–Ω—Ç–∞-–°–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã—Ö –º–æ—Å—Ç–æ–≤...")
        directions = []
        entity_papers = defaultdict(set)
        
        # 1. –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ
        for u, v, data in self.graph.edges(data=True):
            if data.get('type') == 'MENTIONS':
                concept_node = self.graph.nodes[u]
                entity_node = self.graph.nodes[v]
                paper_id = concept_node.get('paper_id')
                entity_name = entity_node.get('name')
                if paper_id and entity_name:
                    entity_papers[entity_name].add(paper_id)

        # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –º–æ—Å—Ç–æ–≤
        bridge_tasks = []
        for entity, papers in entity_papers.items():
            if len(papers) > 1:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —ç—Ç–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö —Å–≤—è–∑–µ–π
                contexts = defaultdict(list)
                for u, v, data in self.graph.edges(data=True):
                    if data.get('type') == 'MENTIONS' and self.graph.nodes[v].get('name') == entity:
                        paper_id = self.graph.nodes[u].get('paper_id')
                        if paper_id in papers and data.get('context'):
                            contexts[paper_id].append(data['context'])
                
                if contexts:
                    bridge_tasks.append({
                        'entity': entity,
                        'contexts': dict(contexts),
                        'papers': list(papers)
                    })
        
        print(f"     üìã –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(bridge_tasks)} –∑–∞–¥–∞—á –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –º–æ—Å—Ç–æ–≤ ({max_workers} –ø–æ—Ç–æ–∫–æ–≤)")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ–∑–∞ –∏–¥–µ–π-–º–æ—Å—Ç–æ–≤
        def process_bridge(task):
            idea = self._synthesize_bridge_idea(task['entity'], task['contexts'])
            if idea:
                return {
                    "type": "Bridge",
                    "title": idea.title,
                    "description": f"{idea.scientific_premise} {idea.proposed_direction}",
                    "supporting_papers": task['papers']
                }
            return None
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {
                executor.submit(process_bridge, task): task 
                for task in bridge_tasks
            }
            
            for future in tqdm(concurrent.futures.as_completed(future_to_task), 
                             total=len(bridge_tasks), desc="–°–∏–Ω—Ç–µ–∑ –∏–¥–µ–π-–º–æ—Å—Ç–æ–≤",
                             position=0, leave=True):
                try:
                    result = future.result()
                    if result:
                        directions.append(result)
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–æ—Å—Ç–∞: {e}")
        return directions

    def _generate_directions_from_new_methods(self, max_workers=4) -> list:
        """–ü–æ–∏—Å–∫ '–Ω–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –ø—Ä–æ–±–ª–µ–º' —Å –ø–æ–º–æ—â—å—é –ê–≥–µ–Ω—Ç–∞-–°–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞"""
        print("  üß™ –ó–∞–ø—É—Å–∫–∞—é –ê–≥–µ–Ω—Ç–∞-–°–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤...")
        directions = []
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π –≥–æ–¥ –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö  
        latest_year = max((data.get('year', 0) for n, data in self.graph.nodes(data=True) 
                          if data.get('type') == 'Paper'), default=2024)
        
        print(f"     üìÖ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –º–µ—Ç–æ–¥—ã –∏–∑ —Å–≤–µ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π ({latest_year} –≥–æ–¥)")
        
        method_entity_pairs = []
        for node_id, data in self.graph.nodes(data=True):
            if (data.get('type') == 'Method' and 
                self.graph.nodes.get(data.get('paper_id', ''), {}).get('year') == latest_year):
                
                for successor in self.graph.successors(node_id):
                    if self.graph.nodes[successor].get('type') == 'Entity':
                        entity_name = self.graph.nodes[successor].get('name')
                        method_entity_pairs.append({
                            'method_text': data.get('statement', data.get('content', 'N/A')),
                            'entity_name': entity_name,
                            'paper_id': data.get('paper_id'),
                            'paper_year': latest_year
                        })
        
        print(f"     üîç –ù–∞–π–¥–µ–Ω–æ {len(method_entity_pairs)} –ø–∞—Ä –º–µ—Ç–æ–¥-—Å—É—â–Ω–æ—Å—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ({max_workers} –ø–æ—Ç–æ–∫–æ–≤)")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤
        def process_new_method(pair):
            idea = self._synthesize_new_method_idea(
                pair['method_text'], 
                pair['entity_name'], 
                pair['paper_id'], 
                pair['paper_year']
            )
            if idea:
                return {
                    "type": "New Tool, Old Problem",
                    "title": idea.title,
                    "description": f"{idea.scientific_premise} {idea.proposed_direction}",
                    "supporting_papers": [pair['paper_id']]
                }
            return None
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {
                executor.submit(process_new_method, task): task 
                for task in method_entity_pairs
            }
            
            for future in tqdm(concurrent.futures.as_completed(future_to_task), 
                             total=len(method_entity_pairs), desc="–ê–Ω–∞–ª–∏–∑ –Ω–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤",
                             position=0, leave=True):
                try:
                    result = future.result()
                    if result:
                        directions.append(result)
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ–≥–æ –º–µ—Ç–æ–¥–∞: {e}")
        
        print(f"     ‚úÖ –°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–æ {len(directions)} –∏–¥–µ–π –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤")
        return directions

    def _synthesize_bridge_idea(self, entity_name: str, contexts: dict) -> SynthesizedBridgeIdea:
        """–í—ã–∑—ã–≤–∞–µ—Ç LLM –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–¥–µ–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤."""
        prompt = f"""# ROLE
You are a perceptive scientific strategist, adept at seeing non-obvious connections between different fields of research.

# TASK
I have discovered a structural link: the entity '{entity_name}' is mentioned in several papers in different contexts. Your task is to analyze these contexts and synthesize a valuable and original research idea from them. Do not state the obvious. Look for a genuine scientific gap.

# CONTEXTS OF MENTION
"""
        for paper_id, context_list in contexts.items():
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π, —Å–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
            prompt += f"- In paper {paper_id}: \"{context_list[0][:500]}...\"\n"

        prompt += """
# INSTRUCTIONS
Generate a title, a scientific premise, and a concrete research proposal. Be concise but compelling. Your response MUST be a JSON object matching the SynthesizedBridgeIdea schema.
"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –º–æ—â–Ω—ã–π –∫–ª–∏–µ–Ω—Ç, —á—Ç–æ –∏ –¥–ª—è –∫—Ä–∏—Ç–∏–∫–∏
            from config import llm_critic_client 
            synthesized_idea = llm_critic_client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                response_model=SynthesizedBridgeIdea
            )
            return synthesized_idea
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ –∏–¥–µ–∏ –¥–ª—è '{entity_name}': {e}")
            return None

    def _synthesize_whitespot_idea(self, hypothesis_text: str, paper_id: str, paper_context: str = "") -> SynthesizedBridgeIdea:
        """–°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –±–µ–ª–æ–≥–æ –ø—è—Ç–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∏–ø–æ—Ç–µ–∑—ã"""
        prompt = f"""# ROLE
You are a strategic research advisor specializing in identifying high-impact validation opportunities in biomedical research.

# TASK
I have identified an untested hypothesis from a scientific paper. Your task is to analyze this hypothesis and craft a compelling research direction that explains WHY validating this hypothesis is scientifically important and HOW it could advance the field.

# HYPOTHESIS DETAILS
Paper ID: {paper_id}
Hypothesis: "{hypothesis_text}"
Additional Context: {paper_context[:500] if paper_context else "Limited context available"}

# INSTRUCTIONS
Analyze the hypothesis and generate:
1. A compelling title that captures the validation opportunity
2. A scientific premise explaining WHY this hypothesis matters (what gap it fills, what it could reveal)
3. A concrete research proposal for HOW to validate it experimentally

Focus on the scientific significance, not just the mechanics. Your response MUST be a JSON object matching the SynthesizedBridgeIdea schema.
"""
        try:
            synthesized_idea = llm_critic_client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                response_model=SynthesizedBridgeIdea
            )
            return synthesized_idea
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ –±–µ–ª–æ–≥–æ –ø—è—Ç–Ω–∞ –¥–ª—è {paper_id}: {e}")
            return None

    def _synthesize_new_method_idea(self, method_text: str, entity_name: str, paper_id: str, paper_year: int) -> SynthesizedBridgeIdea:
        """–°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –∏–¥–µ—é –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –º–µ—Ç–æ–¥–∞ –∫ —Å—Ç–∞—Ä—ã–º –ø—Ä–æ–±–ª–µ–º–∞–º"""
        prompt = f"""# ROLE
You are a translational research strategist, expert at identifying how cutting-edge methodologies can solve longstanding problems in different fields.

# TASK
I have identified a novel methodology from recent research. Your task is to envision how this method could be applied to address well-established, unresolved challenges involving the same biological entity in other contexts.

# METHOD DETAILS
Paper ID: {paper_id} (Year: {paper_year})
Method: "{method_text}"
Target Entity: {entity_name}

# INSTRUCTIONS
Think creatively about:
1. What established, challenging problems exist around {entity_name} that current methods struggle with?
2. How could this novel approach provide a breakthrough solution?
3. What specific old problem could be solved with this new tool?

Generate:
- A title that captures the methodological innovation opportunity
- A scientific premise explaining what old problem this new method could solve
- A concrete proposal for applying the method to that specific challenge

Be specific about the problem being solved, not just the method being applied. Your response MUST be a JSON object matching the SynthesizedBridgeIdea schema.
"""
        try:
            synthesized_idea = llm_critic_client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                response_model=SynthesizedBridgeIdea
            )
            return synthesized_idea
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è {paper_id}: {e}")
            return None

    def generate_research_directions(self, max_workers=4) -> list:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (—Ñ–∞–∑–∞ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è)"""
        all_directions = []
        all_directions.extend(self._generate_directions_from_white_spots(max_workers))
        all_directions.extend(self._generate_directions_from_bridges(max_workers))
        all_directions.extend(self._generate_directions_from_new_methods(max_workers))
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        unique_directions = {d['title']: d for d in all_directions}.values()
        return list(unique_directions)

    def _critique_single_direction(self, direction: dict) -> dict:
        """–ö—Ä–∏—Ç–∏–∫—É–µ—Ç –æ–¥–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏)"""
        PROMPT_CRITIC = """
# ROLE
You are a cynical but fair, world-renowned scientific reviewer for the journal 'Nature'. Your task is to ruthlessly but objectively evaluate the proposed scientific direction. You are looking for true breakthroughs, not incremental improvements.

# TASK
Evaluate the proposed research direction. Your verdict must be structured and based on three pillars: Novelty, Potential Impact, and Feasibility. Don't fall for fancy words, look at the essence.

# PROPOSED DIRECTION:
"{description}"

# EVALUATION INSTRUCTIONS:
1.  **Interest check (is_interesting):** Does this even make sense? If the idea is absurd or trivial, immediately set `false`.
2.  **Novelty (novelty_score):** Is this really something new, or just repackaging old ideas? (10 = new paradigm, 1 = another BERT paper).
3.  **Impact (impact_score):** If this works, will it change the world or just add +0.1% to some benchmark? (10 = Nobel Prize, 1 = nobody will notice).
4.  **Feasibility (feasibility_score):** Can this be tested today or is it science fiction 50 years ahead? (10 = can be done in a year in grad school, 1 = requires building a time machine).
5.  **Final Score (final_score):** Calculate as `0.5*impact + 0.3*novelty + 0.2*feasibility`.
6.  **Strengths:** 1-2 points what can be praised.
7.  **Weaknesses:** 1-2 points where the main risks and problems are.
8.  **Recommendation:** 'Strongly Recommend' (if final_score > 7.5), 'Consider' (if final_score > 5.0), 'Reject' (in all other cases).

CRITICAL: Your response MUST be ONLY a JSON object that corresponds to the Pydantic Critique schema. 
CRITICAL: All text fields (strengths, weaknesses) MUST be in English only.
"""
        
        try:
            critique = llm_critic_client.chat.completions.create(
                messages=[{"role": "user", "content": PROMPT_CRITIC.format(description=direction['description'])}],
                response_model=Critique
            )
            
            if critique.is_interesting:
                direction['critique'] = critique
                return direction
            else:
                return None
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è '{direction['title']}': {e}")
            return None

    def critique_and_prioritize(self, directions: list, max_workers=4) -> list:
        """–ö—Ä–∏—Ç–∏–∫—É–µ—Ç –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (—Ñ–∞–∑–∞ —Å—Ö–æ–∂–¥–µ–Ω–∏—è)"""
        print(f"     üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –∫—Ä–∏—Ç–∏–∫—É {len(directions)} –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π ({max_workers} –ø–æ—Ç–æ–∫–æ–≤)")
        
        critiqued_directions = []
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_direction = {
                executor.submit(self._critique_single_direction, direction): direction 
                for direction in directions
            }
            
            for future in tqdm(concurrent.futures.as_completed(future_to_direction), 
                             total=len(directions), desc="–ö—Ä–∏—Ç–∏–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π",
                             position=0, leave=True):
                try:
                    result = future.result()
                    if result is not None:
                        critiqued_directions.append(result)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {e}")
        
        sorted_directions = sorted(critiqued_directions, key=lambda x: x['critique'].final_score, reverse=True)
        
        final_ranking = [
            PrioritizedDirection(
                rank=i + 1,
                title=direction['title'],
                description=direction['description'],
                critique=direction['critique'],
                supporting_papers=direction['supporting_papers'],
                research_type=direction.get('type', 'Unknown')
            )
            for i, direction in enumerate(sorted_directions)
        ]
        return final_ranking

    def save_report(self, prioritized_list: list, filepath: str = "research_report.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ JSON —Ñ–∞–π–ª"""
        try:
            filepath = Path(filepath)
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è
            report_data = {
                "timestamp": datetime.now().isoformat(),
                "total_directions": len(prioritized_list),
                "directions": []
            }
            
            for direction in prioritized_list:
                direction_data = {
                    "rank": direction.rank,
                    "title": direction.title,
                    "description": direction.description,
                    "supporting_papers": direction.supporting_papers,
                    "critique": {
                        "is_interesting": direction.critique.is_interesting,
                        "novelty_score": direction.critique.novelty_score,
                        "impact_score": direction.critique.impact_score,
                        "feasibility_score": direction.critique.feasibility_score,
                        "final_score": direction.critique.final_score,
                        "strengths": direction.critique.strengths,
                        "weaknesses": direction.critique.weaknesses,
                        "recommendation": direction.critique.recommendation
                    }
                }
                report_data["directions"].append(direction_data)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            print(f"üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filepath}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
            return False

    def _synthesize_cluster_report(self, cluster_directions: list) -> ThematicProgram:
        """–í—ã–∑—ã–≤–∞–µ—Ç –ì–ª–∞–≤–Ω–æ–≥–æ –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ v2.1 –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã —Å –ø–æ–¥–≥—Ä—É–ø–ø–∞–º–∏."""
        
        # –ì–æ—Ç–æ–≤–∏–º –¥–µ—Ç–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        detailed_directions = [
            {
                "rank": d.rank, 
                "title": d.title, 
                "description": d.description,
                "type": d.research_type,  # –¢–∏–ø –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–≤–∏—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                "final_score": d.critique.final_score
            } 
            for d in cluster_directions
        ]

        prompt = f"""# ROLE
You are a Chief Scientific Officer with expertise in structuring complex research portfolios. Your analysts have provided you with a cluster of related research directions. Your task is to create a strategic research program with internal structure.

# TASK
Analyze the following research directions and create a structured strategic program:

1. Write a high-level program title and summary
2. CRITICALLY: Group the directions into 2-4 focused subgroups based on research approach:
   - "Fundamental Mechanism Exploration" (basic science, understanding how things work)
   - "Hypothesis Validation" (testing specific untested claims or predictions)  
   - "Methodological Application" (applying new tools/techniques to solve problems)

Each subgroup should contain 2-8 related directions. Provide a 1-2 sentence description of each subgroup's focus.

# INPUT DATA (CLUSTERED RESEARCH DIRECTIONS)
{json.dumps(detailed_directions, indent=2)}

# INSTRUCTIONS
Your response MUST follow this exact JSON structure:
{{
  "program_title": "Strategic Program Title",
  "program_summary": "2-3 sentence summary of importance and scope",
  "subgroups": [
    {{
      "subgroup_type": "Fundamental Mechanism Exploration",
      "subgroup_description": "Brief focus description",
      "direction_ranks": [1, 3, 5]
    }},
    {{
      "subgroup_type": "Hypothesis Validation", 
      "subgroup_description": "Brief focus description",
      "direction_ranks": [2, 4]
    }}
  ]
}}

CRITICAL: Only use the exact subgroup_type names provided. Include direction_ranks as list of integers.
"""
        try:
            from pydantic import BaseModel
            from typing import List

            class SubgroupStructure(BaseModel):
                subgroup_type: DirectionType
                subgroup_description: str
                direction_ranks: List[int]

            class StructuredProgram(BaseModel):
                program_title: str
                program_summary: str
                subgroups: List[SubgroupStructure]

            response = llm_critic_client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                response_model=StructuredProgram
            )
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –ø–æ –ø–æ–¥–≥—Ä—É–ø–ø–∞–º
            final_subgroups = []
            rank_to_direction = {d.rank: d for d in cluster_directions}
            
            for subgroup in response.subgroups:
                subgroup_directions = []
                for rank in subgroup.direction_ranks:
                    if rank in rank_to_direction:
                        subgroup_directions.append(rank_to_direction[rank])
                
                if subgroup_directions:  # –¢–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ –ø–æ–¥–≥—Ä—É–ø–ø—ã
                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –ø–æ–¥–≥—Ä—É–ø–ø—ã –ø–æ —Ä–∞–Ω–≥—É (–ª—É—á—à–∏–µ —Å–Ω–∞—á–∞–ª–∞)
                    subgroup_directions.sort(key=lambda d: d.rank)
                    final_subgroups.append(DirectionSubgroup(
                        subgroup_type=subgroup.subgroup_type,
                        subgroup_description=subgroup.subgroup_description,
                        directions=subgroup_directions
                    ))
            
            return ThematicProgram(
                program_title=response.program_title,
                program_summary=response.program_summary,
                subgroups=final_subgroups
            )
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã: {e}")
            return None

    def _get_gemini_embeddings(self, texts: list) -> np.ndarray:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ Gemini API"""
        try:
            from google import genai
            import os
            import numpy as np
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç Gemini –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            client = genai.Client()
            
            print(f"      üî¢ –ü–æ–ª—É—á–∞—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {len(texts)} –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ Gemini...")
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            result = client.models.embed_content(
                model="gemini-embedding-001",
                contents=texts  # contents, –Ω–µ content!
            )
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array –¥–ª—è sklearn
            embeddings_list = []
            for embedding in result.embeddings:  # result.embeddings, –Ω–µ result['embedding']
                embeddings_list.append(embedding.values)  # embedding.values –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–∏—Å–µ–ª
            
            embeddings_array = np.array(embeddings_list)
            print(f"      ‚úÖ –ü–æ–ª—É—á–µ–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ {embeddings_array.shape}")
            
            return embeddings_array
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ Gemini: {e}")
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            print("   üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
            return np.random.rand(len(texts), 768)

    def analyze_and_synthesize_report(self, directions: list, max_workers=4) -> HierarchicalReport:
        """–ù–æ–≤—ã–π –≥–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥, –≤–∫–ª—é—á–∞—é—â–∏–π –∫—Ä–∏—Ç–∏–∫—É, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∏ —Å–∏–Ω—Ç–µ–∑."""
        
        # 1. –ö—Ä–∏—Ç–∏–∫—É–µ–º –≤—Å–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ
        print("   üéØ -> Phase 2.1: Critiquing and prioritizing directions...")
        critiqued_list = self.critique_and_prioritize(directions, max_workers)

        if not critiqued_list:
            return HierarchicalReport(timestamp=datetime.now().isoformat(), total_programs=0, programs=[], unclustered_directions=[])
        
        # 2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        print("   üß† -> Phase 2.2: Clustering directions thematically...")
        from sklearn.cluster import DBSCAN
        import numpy as np
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemini embeddings –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤
        embeddings = self._get_gemini_embeddings([d.description for d in critiqued_list])

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º DBSCAN –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—Å—Ç—Ä–æ–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)
        dbscan = DBSCAN(eps=0.35, min_samples=2, metric='cosine')
        clusters = dbscan.fit_predict(embeddings)

        clustered_directions = defaultdict(list)
        unclustered_directions = []
        for i, direction in enumerate(critiqued_list):
            if clusters[i] == -1: # -1 —ç—Ç–æ —à—É–º (–Ω–µ –ø–æ–ø–∞–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä)
                unclustered_directions.append(direction)
            else:
                clustered_directions[clusters[i]].append(direction)
        
        print(f"      ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(clustered_directions)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.")

        # 3. –°–∏–Ω—Ç–µ–∑ –æ—Ç—á–µ—Ç–∞ –ì–ª–∞–≤–Ω—ã–º –ê–Ω–∞–ª–∏—Ç–∏–∫–æ–º
        print("   üèÜ -> Phase 2.3: Synthesizing the final strategic report...")
        final_programs = []
        for cluster_id, directions_in_cluster in tqdm(clustered_directions.items(), 
                                                     desc="–°–∏–Ω—Ç–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º",
                                                     position=0, leave=True):
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–¥–µ–∏ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ —Ä–∞–Ω–≥—É
            sorted_directions = sorted(directions_in_cluster, key=lambda d: d.rank)
            program = self._synthesize_cluster_report(sorted_directions)
            if program:
                final_programs.append(program)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∞–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –ª—É—á—à–µ–º—É —Å–∫–æ—Ä—É –≤–Ω—É—Ç—Ä–∏)
        final_programs.sort(key=lambda p: max(d.critique.final_score for d in p.component_directions), reverse=True)

        return HierarchicalReport(
            timestamp=datetime.now().isoformat(),
            total_programs=len(final_programs),
            programs=final_programs,
            unclustered_directions=sorted(unclustered_directions, key=lambda d: d.rank)
        )

    def save_hierarchical_report(self, report: HierarchicalReport, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –≤ JSON —Ñ–∞–π–ª"""
        try:
            with open(filepath, "w", encoding='utf-8') as f:
                f.write(report.model_dump_json(indent=2))
            print(f"üíæ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {filepath}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {e}")
            return False 